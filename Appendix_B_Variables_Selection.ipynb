{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tWJCDH-R1R9"
      },
      "source": [
        "# Appendix B: Variables selection\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Packages import and installation\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "\n",
        "%pip install matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%pip install seaborn\n",
        "import seaborn as sns\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from pandas.plotting import scatter_matrix\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "import scipy.stats as stats\n",
        "\n",
        "pd.set_option('display.max_rows', 25)"
      ],
      "metadata": {
        "id": "9WrPs6OLHdGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npAwlkofGKfX"
      },
      "outputs": [],
      "source": [
        "# Reading the cleaned and merged (Targe+Non-target) dataset into a datafre. This dataset was pickled and named 'Merged numerical financial dataset' in Appendix A \n",
        "\n",
        "num_df = pd.read_pickle(\"Merged numerical financial dataset\")\n",
        "\n",
        "# Visualizing the DataFrame\n",
        "num_df.head()\n",
        "num_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CID4TfIgR1Sf"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Dropping the unneccessary columns, to make my dataset ready for the filter methods\n",
        "\n",
        "num_df_filter = num_df.drop(['Index','Instrument','AD', 'AD-30'], axis=1)\n",
        "num_df_filter.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZfPHgVlR1Sm"
      },
      "source": [
        "# APPLYING FILTER METHODS FOR VARIABLES SELECTION\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JF2dKmrUR1Sm"
      },
      "outputs": [],
      "source": [
        "# Getting the dataset ready, by splitting into test and train\n",
        "\n",
        "x = num_df_filter\n",
        "y = num_df_filter['Target/Non-Target']\n",
        "\n",
        "trainX, testX, trainY, testY = train_test_split(x, y, test_size = 0.2, random_state=45)\n",
        "\n",
        "# Next we will use VarianceThreshold function to remove quasi constant features.\n",
        "\n",
        "constant_filter = VarianceThreshold(threshold=0)\n",
        "constant_filter.fit(trainX)\n",
        "\n",
        "# Similarly, to find the number of constant features the following code was used:\n",
        "\n",
        "constant_columns = [column for column in trainX.columns\n",
        "                    if column not in trainX.columns[constant_filter.get_support()]]\n",
        "\n",
        "print(len(constant_columns))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqpumCDaR1Sm"
      },
      "outputs": [],
      "source": [
        "# Now we want to get all the features that are not constant (features we want to keep):\n",
        "\n",
        "len(trainX.columns[constant_filter.get_support()]) # All columns appear to be non constant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xEpVitOR1Sm"
      },
      "outputs": [],
      "source": [
        "# Removing Quasi-Constant features\n",
        "\n",
        "qconstant_filter = VarianceThreshold(threshold=0.01)\n",
        "qconstant_filter.fit(trainX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3GPRb-vR1Sm"
      },
      "outputs": [],
      "source": [
        "# Identify the Quasi constant features\n",
        "\n",
        "qconstant_columns = [column for column in trainX.columns\n",
        "                    if column not in trainX.columns[qconstant_filter.get_support()]]\n",
        "\n",
        "print(len(qconstant_columns)) # 0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RESULTS**: No constant features nor duplicated features were identified in the dataset."
      ],
      "metadata": {
        "id": "sTyPWlmovIcR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VARIABLE SELECTION WITH A TRIANGULATION METHOD (PEARSON, SPEARMAN & t-test)"
      ],
      "metadata": {
        "id": "5v9v_KlZvt3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping the unneccessary columns\n",
        "\n",
        "num_df_noindex = num_df.drop(['Index', 'Target/Non-Target','Instrument','AD', 'AD-30'], axis=1)"
      ],
      "metadata": {
        "id": "4R8gQUIztIG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-Fge0mKR1Sf"
      },
      "outputs": [],
      "source": [
        "# Using Scikit-learn to transform the dataset with maximum absolute scaling, for better visualization\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(num_df_noindex)\n",
        "scaled = scaler.transform(num_df_noindex)\n",
        "scaled_df = pd.DataFrame(scaled, columns=num_df_noindex.columns)\n",
        "\n",
        "scaled_df.head()\n",
        "scaled_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxGCYv9mR1Sg"
      },
      "outputs": [],
      "source": [
        "# Crating a Scatter Matrix plot of the dataset\n",
        "\n",
        "axes = pd.plotting.scatter_matrix(scaled_df, figsize  = [20, 20], alpha=0.2)\n",
        "for ax in axes.flatten():\n",
        "    ax.xaxis.label.set_rotation(90)\n",
        "    ax.yaxis.label.set_rotation(0)\n",
        "    ax.yaxis.label.set_ha('right')\n",
        "\n",
        "plt.suptitle('Scatter Matrix representation of the Variables', y=1, fontsize=25)\n",
        "\n",
        "# Setting the layout\n",
        "plt.tight_layout()\n",
        "plt.gcf().subplots_adjust(wspace=0.2, hspace=0.2)\n",
        "\n",
        "# Displaying the title\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiSoIXiGR1Sh"
      },
      "outputs": [],
      "source": [
        "# Calculating Pearson's correlation coefficient \n",
        "\n",
        "num_df_noindex.corr()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BKnFkBDR1Sh"
      },
      "outputs": [],
      "source": [
        "# Plotting the results into a Heatmap\n",
        "\n",
        "# Define figure size\n",
        "\n",
        "plt.figure(figsize=(22,20))\n",
        "\n",
        "# Define title and dimensions of the words\n",
        "plt.title(\"Pearson Correlation Coefficient Heatmap\", y=1, fontsize=25)\n",
        "\n",
        "# Printing a heat map of the correlation\n",
        "\n",
        "matrix = num_df_noindex.corr().round(2)\n",
        "sns.heatmap(matrix, annot=True, vmax=1, vmin=-1, center=0, cmap='vlag')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7HnXp1NJR1Sh"
      },
      "outputs": [],
      "source": [
        "# Selecting Only Strong Correlations in the Correlation Matrix (Checking also for negative correlation)\n",
        "\n",
        "Matrix_strong = num_df_noindex.corr()\n",
        "Matrix_strong = Matrix_strong.unstack()\n",
        "Matrix_strong = Matrix_strong[abs(Matrix_strong) >= 0.7]\n",
        "\n",
        "\n",
        "Most_correlated = Matrix_strong.sort_values(ascending=False)\n",
        "Most_correlated\n",
        "pd.set_option('display.max_rows', 10000)\n",
        "\n",
        "\n",
        "print(Most_correlated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pv8T1I6OR1Si"
      },
      "outputs": [],
      "source": [
        "# Most correlated variables with score greater than 0.7\n",
        "\n",
        "correlated_features = set()\n",
        "threshold = 0.70\n",
        "\n",
        "for i in range(len(matrix.columns)):\n",
        "    for j in range(i):\n",
        "        if abs(matrix.iloc[i, j]) >= threshold:\n",
        "            colname = matrix.columns[i]\n",
        "            correlated_features.add(colname)\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SoHmDCoR1Si"
      },
      "outputs": [],
      "source": [
        "print(correlated_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8HxQFPhR1Si"
      },
      "source": [
        "**RESULTS**: Most correlated variables according to Pearson results, by number of variables correlation:\n",
        "\n",
        "9\tTotal Capital /\n",
        "9\tTotal Shareholders' Equity incl Minority Intr & Hybrid Debt /\n",
        "8\tRevenue from Business Activities - Total.2 /\n",
        "8\tRevenue from Business Activities - Total.1 /\n",
        "8\tEnterprise Value (Daily Time Series) /\n",
        "8\tCash & Cash Equivalents - Total /\n",
        "8\tRevenue from Business Activities - Total.3 /\n",
        "8\tRevenue from Business Activities - Total / \n",
        "8\tDebt - Total /\n",
        "4\tEarnings before Interest Taxes Depreciation & Amortization /\n",
        "2\tFree Cash Flow /\n",
        "1\tPrice To Sales Per Share (Daily Time Series Ratio) /\n",
        "1\tOperating Margin - %, TTM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7HN-A-bR1Sj"
      },
      "outputs": [],
      "source": [
        "# Calculating Spearman’s correlation\n",
        "\n",
        "Matrix_Spearman = num_df_noindex.corr(method=\"spearman\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2nmJIojR1Sj"
      },
      "outputs": [],
      "source": [
        "# Showing Spearman's coefficients on a Heatmap\n",
        "\n",
        "# Defining figure size\n",
        "\n",
        "plt.figure(figsize=(22,20))\n",
        "\n",
        "# Defining title\n",
        "\n",
        "plt.title(\"Spearman Correlation Coefficient Heatmap\", y=1, fontsize=25)\n",
        "\n",
        "# Printing a heat map of the correlation\n",
        "\n",
        "sns.heatmap(Matrix_Spearman, annot=True, vmax=1, vmin=-1, center=0, cmap='vlag')\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgUL7kG2R1Sj"
      },
      "outputs": [],
      "source": [
        "# Selecting only the most correlated variables (negatively and positively)\n",
        "\n",
        "Matrix_P_strong = num_df_noindex.corr(method=\"spearman\")\n",
        "Matrix_P_strong = Matrix_P_strong.unstack()\n",
        "Matrix_P_strong = Matrix_P_strong[abs(Matrix_P_strong) >= 0.7]\n",
        "\n",
        "\n",
        "Most_correlated_P = Matrix_P_strong.sort_values(ascending=False)\n",
        "Most_correlated_P\n",
        "\n",
        "print(Most_correlated_P)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnByYBIoR1Sj"
      },
      "outputs": [],
      "source": [
        "# Check with Spearman as well! The most correlated values\n",
        "\n",
        "Spearman_correlated_features = set()\n",
        "threshold = 0.70\n",
        "\n",
        "for i in range(len(Matrix_Spearman.columns)):\n",
        "    for j in range(i):\n",
        "        if abs(Matrix_Spearman.iloc[i, j]) >= threshold:\n",
        "            colname = Matrix_Spearman.columns[i]\n",
        "            Spearman_correlated_features.add(colname)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYrTvs_YR1Sj"
      },
      "outputs": [],
      "source": [
        "print(Spearman_correlated_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RESULTS**: Most correlated variables according to Spearman results, by number of variables correlation:\n",
        "\n",
        "9 Total Capital /\n",
        "9 Revenue from Business Activities - Total /\n",
        "9 Revenue from Business Activities - Total.3 /\n",
        "9 Revenue from Business Activities - Total.1 /\n",
        "9 Enterprise Value (Daily Time Series) /\n",
        "8 Earnings before Interest Taxes Depreciation & Amortization /\n",
        "8 Total Shareholders' Equity incl Minority Intr & Hybrid Debt /\n",
        "8 Debt - Total /\n",
        "6 Cash & Cash Equivalents - Total\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ctveI0B8eJTN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duuujIKyR1Sj"
      },
      "source": [
        "# Calculating t-test, to be used on Pearsman coefficients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCCQfyONR1Sk"
      },
      "outputs": [],
      "source": [
        "# Getting the dataset with the Terget column included,so I can apply t-test on each column, comparing target and non target values\n",
        "\n",
        "num_df_target = num_df.drop(['Index', 'Instrument','AD-30','AD'], axis=1)\n",
        "num_df_target.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the dataset with only the highly correlated variables\n",
        "\n",
        "Correlated_T_test = num_df_target.drop(num_df_target.columns[[1, 2, 4, 5, 10, 11, 12, 17]], axis=1) # With Target and no index"
      ],
      "metadata": {
        "id": "Q2B5Ahq2cB7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Correlated_T_test.head()"
      ],
      "metadata": {
        "id": "3gd3DQLddFmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5k0hamDRR1Sk"
      },
      "outputs": [],
      "source": [
        "# Running the T-test to help me decide which variable to remove\n",
        "\n",
        "\n",
        "T_test = stats.ttest_ind(Correlated_T_test.loc[Correlated_T_test['Target/Non-Target']==1], Correlated_T_test.loc[Correlated_T_test['Target/Non-Target']==0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "6T_UCeLgR1Sk"
      },
      "outputs": [],
      "source": [
        "# Storing the calculated results in a dataframe, so we can visualize it:\n",
        "\n",
        "T_test_data = pd.DataFrame()\n",
        "T_test_data[\"Variable\"] = Correlated_T_test.columns # Setting the Variables columns\n",
        "T_test_data[\"T_test\"] = T_test[0] # Setting the T-test values values per column\n",
        "T_test_data[\"p-value\"] = T_test[1] # Setting the p-value values per column\n",
        "\n",
        "# Creating the acuat dataframe:\n",
        "T_test_data = T_test_data.T\n",
        "T_test_data.rename(columns = T_test_data.iloc[0], inplace = True)\n",
        "T_test_data = T_test_data.iloc[1:]\n",
        "\n",
        "# Printing the outcome:\n",
        "T_test_data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RESULTS**: The only variable which was kept in the original dataset and not dropped, was ‘Debt-Total’: with a p value of .7425, there was a high 74.25% chance that the relationship, and therefore high correlation, occurred by chance. Since this p-value was not less than .05, we failed to reject the null hypothesis, so in this case, with a high p value and lower T-test the relationship was deemed as not statistically significant as the data didn’t allow to reject the null hypothesis and didn’t provide support for the alternative hypothesis. The decision to keep the variable was then made.\n",
        "The threshold for the p-value was set at .7425, so all the other variables were deemed statistically significant and dropped.\n"
      ],
      "metadata": {
        "id": "K1dm63FIneib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping unneccessary columns\n",
        "\n",
        "num_df_features = num_df.drop(num_df.columns[[3,4,7,10,11,12,13,17,18,19,22]], axis=1) # With Target\n",
        "num_df_features.head()"
      ],
      "metadata": {
        "id": "EHUxd0YjU4IQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the columns\n",
        "\n",
        "num_df_features.columns"
      ],
      "metadata": {
        "id": "NTIxodl1q47x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRvBvJFxR1Sl"
      },
      "outputs": [],
      "source": [
        "# Cheching the dataframe shape\n",
        "\n",
        "num_df_features.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exporting the final, variable selected dataframe with pickle, so we can use this dataset later for further processing\n",
        "\n",
        "num_df_features.to_pickle(\"Variable selected financial dataset\")"
      ],
      "metadata": {
        "id": "TytmoVjLwBCx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}