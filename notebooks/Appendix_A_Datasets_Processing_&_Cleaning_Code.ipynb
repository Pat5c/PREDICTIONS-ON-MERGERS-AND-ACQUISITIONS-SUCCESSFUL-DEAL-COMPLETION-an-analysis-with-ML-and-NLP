{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tWJCDH-R1R9"
      },
      "source": [
        "# Appendix A: Datasets Processing & Cleaning Code\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "-LdY_DZZR1SG"
      },
      "outputs": [],
      "source": [
        "# Packages import and installation\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "\n",
        "%pip install matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%pip install seaborn\n",
        "import seaborn as sns\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "from pandas.plotting import scatter_matrix\n",
        "\n",
        "import scipy.stats as stats\n",
        "\n",
        "%pip install nltk\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import FreqDist\n",
        "import re\n",
        "nltk.download('punkt')\n",
        "\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "pd.set_option('display.max_rows', 25)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzB4x8vhR1SH"
      },
      "source": [
        "## Data Pre-processing and Cleaning on the Target Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "P38JdvftR1SI"
      },
      "outputs": [],
      "source": [
        "# Reading the Target dataset, stored as an Excel file\n",
        "\n",
        "target_data = pd.read_excel(\"target_data.xlsx\")\n",
        "target_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "pwp8W9YKR1SI"
      },
      "outputs": [],
      "source": [
        "# Uploading the dataset into a data frame, for additional processing\n",
        "\n",
        "df = pd.read_excel(\"target_data.xlsx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "T-_yD2oaR1SI"
      },
      "outputs": [],
      "source": [
        "# Checking the dataset column types\n",
        "\n",
        "print(df.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "mFjNZm5tR1SJ"
      },
      "outputs": [],
      "source": [
        "# Renaming the first columns as 'Index' instead of unnamed:\n",
        "\n",
        "df.rename(columns = {\"Unnamed: 0\": \"Index\"}, inplace = True)\n",
        "\n",
        "print(df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "4Btyg044R1SK"
      },
      "outputs": [],
      "source": [
        "# Getting some statistics on the dataset\n",
        "\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "V-Gl-fDDR1SK"
      },
      "outputs": [],
      "source": [
        "# Are there any cells with NaN/Na value or any empty cells? \n",
        "\n",
        "df.isnull().values.any() # This checks for NaN/Na and empty cells in the entire DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtSwsUbZR1SL"
      },
      "source": [
        "**RESULTS**: There are no cells with Nan or Na value or empty cells in this dataset, next, the dataset is going to be checked for \n",
        "any other String values, cells with value 0 and for duplicated rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "IDKFCUGYR1SL"
      },
      "outputs": [],
      "source": [
        "# Are all the columns, numeric columns? \n",
        "\n",
        "df.apply(lambda s: pd.to_numeric(s, errors='coerce').notnull().all())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3BcsUuFR1SL"
      },
      "source": [
        "**RESULTS**: Column 'Instrument' appears to be a string column. This is correct because this column refers to the referencing code system used to identify the company.\n",
        "All the other columns have numerical values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "cVwlzTCAR1SL"
      },
      "outputs": [],
      "source": [
        "# Are there cells with value 0?\n",
        "\n",
        "df.eq(0).sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0AQRY-2R1SL"
      },
      "source": [
        "**RESULTS**: It appears that there are 3 columns with cells with value 0. These columns are 'Operating Margin - %, TTM' , 'Revenue from Business Activities - Total.3' and 'Debt - Total'. A total of 7 rows with value 0 overall in these column have been identified.\n",
        "To create a complete and comprehensive dataset, these rows are dropped as the outcome is to avoid null values to affect the results. Finally, no duplicated rows are found in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "lbSxSUQeR1SS"
      },
      "outputs": [],
      "source": [
        "# In column 'Operating Margin - %, TTM' which rows exactly contain value 0? \n",
        "\n",
        "Operating_margin = df['Operating Margin - %, TTM'] == 0\n",
        "\n",
        "Operating_margin_True = df[Operating_margin]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "VzuWMl8CR1ST"
      },
      "outputs": [],
      "source": [
        "# In column 'Revenue from Business Activities - Total.3' which rows exactly contain value 0? \n",
        "\n",
        "Revenue_business = df['Revenue from Business Activities - Total.3'] == 0\n",
        "\n",
        "Revenue_business_True = df[Revenue_business]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "vOsw_AU4R1ST"
      },
      "outputs": [],
      "source": [
        "# In column 'Debt - Total' which rows exactly contain value 0? \n",
        "\n",
        "Debt_total = df['Debt - Total'] == 0\n",
        "\n",
        "Debt_total_True = df[Debt_total]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Z7hXAVsiR1ST"
      },
      "outputs": [],
      "source": [
        "# Creating a new dataframe which excludes these rows with value 0, by dropping the not needed rows and creating a new cleaned df\n",
        "\n",
        "new_df = df.drop([109, 155, 71, 2, 14, 26, 189], axis=0)\n",
        "new_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "BlbYpbzcR1ST"
      },
      "outputs": [],
      "source": [
        "# Double checking that no 0 zalues are left in the new dataframe\n",
        "\n",
        "new_df.eq(0).sum() # Cleaned!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "WfObGatzR1ST"
      },
      "outputs": [],
      "source": [
        "# Are there duplicated rows across all columns in the new dataframe? \n",
        "\n",
        "Duplicated_Rows = new_df[new_df.duplicated()]\n",
        "\n",
        "Duplicated_Rows # No!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSFXPRX3R1SU"
      },
      "outputs": [],
      "source": [
        "# Adding a Column Target/Non-Target, with number 1 to indicate the row belongs to the Target dataset and for later on classification task\n",
        "\n",
        "new_df.insert(0, 'Target/Non-Target', '1')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_df"
      ],
      "metadata": {
        "id": "mTl4ZvLUVGRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXK1-YNdR1SU"
      },
      "source": [
        "## Data Pre-processing and Cleaning on the Non-Target Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "NaFFiMrBR1SU"
      },
      "outputs": [],
      "source": [
        "# Uploading the Non-Target dataset into a data fram\n",
        "\n",
        "df2 = pd.read_excel(\"peer_data.xlsx\")\n",
        "\n",
        "df2.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "5SWWNwqDR1SU"
      },
      "outputs": [],
      "source": [
        "# Checking the dataset column names and types:\n",
        "\n",
        "print(df2.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "d1t8YwTkR1SV"
      },
      "outputs": [],
      "source": [
        "# Renaming the first columns as 'Index' instead of unnamed:\n",
        "\n",
        "df2.rename(columns = {\"Unnamed: 0\": \"Index\"}, inplace = True)\n",
        "\n",
        "print(df2.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "2tfZvsToR1SV"
      },
      "outputs": [],
      "source": [
        "# Checking here if the 2 initial dataframes (Target and Non-Target) have actually the same columns:\n",
        "\n",
        "set(df2.columns).intersection(set(new_df.columns))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivaQcbgTR1SV"
      },
      "source": [
        "**RESULTS**: It appears there are 21  columns instead of 22. This is due to the fact that the date limit colum 'AD' is not present as the dataset comprises data from until the time of extraction (December 2021)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "C_8FSz4kR1SV"
      },
      "outputs": [],
      "source": [
        "# Getting some statistics on the dataset\n",
        "\n",
        "df2.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "a2d8f3clR1Sc"
      },
      "outputs": [],
      "source": [
        "# Are there cells with NaN/Na value or any empty cells? \n",
        "\n",
        "df2.isnull().values.any() # This checks for NaN/Na and empty cells in the entire DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "c9Ba6xmQR1Sc"
      },
      "outputs": [],
      "source": [
        "# Are all the columns, numerical columns? \n",
        "\n",
        "df2.apply(lambda s: pd.to_numeric(s, errors='coerce').notnull().all())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbmFGHu-R1Sc"
      },
      "source": [
        "**RESULTS**: Again, column Instrument appears to be a string column. Now, also column 'AD-30, is not considered numerical. Let's have a look at the column type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "flbZP1EjR1Sc"
      },
      "outputs": [],
      "source": [
        "# Checking the dataset columns types\n",
        "\n",
        "print(df2.dtypes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X51tjON7R1Sc"
      },
      "source": [
        "**RESULTS**: The 'AD-30' Column seems to be cathegoriesed as object. We will change it to the more accurate type: date-time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "cwBwEbHwR1Sc"
      },
      "outputs": [],
      "source": [
        "# Converting the AD-30 column into column type datetime64\n",
        "\n",
        "df2['AD-30']= pd.to_datetime(df2['AD-30'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "bOeNP_QwR1Sd"
      },
      "outputs": [],
      "source": [
        "# Checking again the dataset columns types to see if everything is in order here\n",
        "\n",
        "print(df2.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "GwJHQSBXR1Sd"
      },
      "outputs": [],
      "source": [
        "# Running again the numeric column code. Are the columns all numeric now? ( Except for column Instrument)\n",
        "\n",
        "df2.apply(lambda s: pd.to_numeric(s, errors='coerce').notnull().all())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RESULTS**: All good now!"
      ],
      "metadata": {
        "id": "KzffQ33awZgM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "s6W6q4PiR1Sd"
      },
      "outputs": [],
      "source": [
        "# Are there cells with value 0?\n",
        "\n",
        "df2.eq(0).sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mn2s4k4QR1Sd"
      },
      "source": [
        "**RESULTS**: It appears that there are 6 columns with cells with value 0. These columns are 'Index' which is due to the fact that it starts at 0 instead of 1, which is okay. Next, we have the 'Revenue from Business Activities - Total.1' , 'Revenue from Business Activities - Total.2', 'Revenue from Business Activities - Total.3', 'Debt - Total' and 'Cash & Cash Equivalents - Total' columns.\n",
        "\n",
        "A total of 414 rows with value 0 overall in these column have been identified. Again, to create a complete and comprehensive dataset, these rows are dropped.\n",
        "Finally, no duplicated rows are found in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "TNNjNOozR1Sd"
      },
      "outputs": [],
      "source": [
        "# In column 'Revenue from Business Activities - Total.1' which rows exactly contain value 0? \n",
        "\n",
        "Revenue_1 = df2['Revenue from Business Activities - Total.1'] == 0\n",
        "\n",
        "Revenue_1_True = df2[Revenue_1]\n",
        "\n",
        "\n",
        "# In column 'Revenue from Business Activities - Total.2' which rows exactly contain value 0? \n",
        "\n",
        "Revenue_2 = df2['Revenue from Business Activities - Total.2'] == 0\n",
        "\n",
        "Revenue_2_True = df2[Revenue_2]\n",
        "\n",
        "\n",
        "# In column 'Revenue from Business Activities - Total.3' which rows exactly contain value 0? \n",
        "\n",
        "Revenue_3 = df2['Revenue from Business Activities - Total.3'] == 0\n",
        "\n",
        "Revenue_3_True = df2[Revenue_3]\n",
        "\n",
        "\n",
        "# In column 'Debt - Total' which rows exactly contain value 0? \n",
        "\n",
        "Debt = df2['Debt - Total'] == 0\n",
        "\n",
        "Debt_True = df2[Debt]\n",
        "\n",
        "\n",
        "# In column 'Cash & Cash Equivalents - Total' which rows exactly contain value 0? \n",
        "\n",
        "Cash = df2['Cash & Cash Equivalents - Total'] == 0\n",
        "\n",
        "Cash_True = df2[Cash]\n",
        "\n",
        "\n",
        "# Dropping all the above rows with values 0 and creating a new dataframe\n",
        "\n",
        "df2_new = pd.concat([df2, Revenue_1_True, Revenue_2_True, Revenue_3_True, Debt_True, Cash_True]).drop_duplicates(keep=False)\n",
        "\n",
        "\n",
        "# Checking the number of rows and columns\n",
        "\n",
        "df2_new.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "dZShJoiZR1Sd"
      },
      "outputs": [],
      "source": [
        "# Are there cells with value 0 in this new dataset (except for the Index)?\n",
        "\n",
        "df2_new.eq(0).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "x5MNVxlaR1Se"
      },
      "outputs": [],
      "source": [
        "# Are there duplicated rows across all columns in the new dataframe? \n",
        "\n",
        "Duplicated_Rows_2 = df2_new[df2_new.duplicated()]\n",
        "\n",
        "Duplicated_Rows_2 # No!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "moZuREHsR1Se"
      },
      "outputs": [],
      "source": [
        "# Adding a Column Target/Non-Target, with number 0 to indicate the row belongs to the Non Target dataset\n",
        "\n",
        "df2_new.insert(0, 'Target/Non-Target', '0')\n",
        "df2_new.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCqTBR8fR1Se"
      },
      "source": [
        "## Now merging the 2 datasets (Target and Non-Target) together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Pt2KKqMPR1Se"
      },
      "outputs": [],
      "source": [
        "# Merging the target dataset new_df, with the non target df2_new + fixing the column type of the Terget column as integer\n",
        "\n",
        "num_dataset = new_df.append(df2_new)\n",
        "num_dataset[\"Target/Non-Target\"] = num_dataset[\"Target/Non-Target\"].astype(str).astype(int)\n",
        "num_dataset.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the columnn and rows numbers\n",
        "num_dataset.shape"
      ],
      "metadata": {
        "id": "RXBqO3CEyvSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exporting the outcome into pickle, so we can use this dataset later, for further processing\n",
        "\n",
        "num_dataset.to_pickle(\"Merged numerical financial dataset\")"
      ],
      "metadata": {
        "id": "3VfBzfumEfrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCy3JN_5z_cD"
      },
      "source": [
        "## Data Pre-processing and Cleaning on the Headlines Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsjD8RkBzNLZ"
      },
      "outputs": [],
      "source": [
        "# Uploading the dataset into a data frame, let's have a look:\n",
        "\n",
        "df3 = pd.read_excel(\"headlines.xlsx\")\n",
        "\n",
        "# Let's have a look at all the columns\n",
        "\n",
        "df3.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ohvVgnazNLa"
      },
      "outputs": [],
      "source": [
        "# Are there missing values in each of the columns?\n",
        "\n",
        "df3.isnull().any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RA7oA8GwzNLb"
      },
      "outputs": [],
      "source": [
        "# Checking the dataset column types\n",
        "\n",
        "print(df3.dtypes)\n",
        "\n",
        "# Renaming the first columns as 'Index' instead of unnamed:\n",
        "\n",
        "df3.rename(columns = {\"Unnamed: 0\": \"Index\"}, inplace = True)\n",
        "\n",
        "print(df3.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItJPvYDwzNLb"
      },
      "outputs": [],
      "source": [
        "# Removing all special characters using regex, from column 'Headlines'. \n",
        "\n",
        "df3['Headlines'] = df3.Headlines.replace(r'[^\\w\\s]|_', '', regex=True)\n",
        "\n",
        "# Adding homogenity by lowering all the letters\n",
        "\n",
        "df3['Headlines'] = df3.Headlines.str.lower()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDW6EyWVzNLc"
      },
      "outputs": [],
      "source": [
        "# Removing stop words using the function defined below\n",
        "\n",
        "# Getting all the english stopwords list first\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Defining my cleaning function, by first tokenizing the text in column Headlines:\n",
        "\n",
        "def Cleaning_function(Headlines):\n",
        "    '''Tokenizing the text, removing any stop words and returning\n",
        "    the cleaned tokenized sentence'''\n",
        "    word_tokens = word_tokenize(Headlines)\n",
        "    \n",
        "    Cleaned_sentence = []\n",
        "    for word_token in word_tokens:\n",
        "        if word_token not in stop_words:\n",
        "            Cleaned_sentence.append(word_token)\n",
        "            \n",
        "# Joining the cleaned sentences together and returning my clean text:\n",
        "\n",
        "    text = (' '.join(Cleaned_sentence))\n",
        "    return text\n",
        "\n",
        "# Now applying this newly created clean function into my dataframe with apply() to column Headlines\n",
        "\n",
        "df3['Headlines'] = df3['Headlines'].apply(Cleaning_function)\n",
        "\n",
        "# Printing the results:\n",
        "\n",
        "print(df3['Headlines'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNmqM-cozNLe"
      },
      "outputs": [],
      "source": [
        "# Transforming the headlines column into strings so I can perform Frequency distribution:\n",
        "\n",
        "Text = df3['Headlines']\n",
        "\n",
        "txt_string = Text.to_string()\n",
        "\n",
        "# But first, counting the vocabularies in my Text in string format\n",
        "\n",
        "print(len(txt_string)) # 17.888.039 items counted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGimDFuxzNLf"
      },
      "outputs": [],
      "source": [
        "# Plotting the Frequency distribution of the text keywords.\n",
        "# But first, removing the punctuation again. \n",
        "\n",
        "txt_string_nopunct = re.sub(r'[^\\w\\s]', '', txt_string)\n",
        "txt_string_nopunct\n",
        "\n",
        "# Separating the words with a space\n",
        "\n",
        "text_list = txt_string.split(\" \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYrZDxvdzNLg"
      },
      "outputs": [],
      "source": [
        "# Now plotting\n",
        "\n",
        "# Getting the frequency distribution list \n",
        "\n",
        "freqDist = FreqDist(text_list)\n",
        "\n",
        "# Printing the 15 most common keywords\n",
        "\n",
        "print(freqDist.most_common(15))\n",
        "\n",
        "# Creating FreqDist for these 15 most common keywords\n",
        "\n",
        "freqDist = FreqDist(text_list).most_common(15)\n",
        "\n",
        "# Converting the above to Pandas series via Python Dictionary for easier plotting\n",
        "\n",
        "freqDist = pd.Series(dict(freqDist))\n",
        "\n",
        "# Removing the empty space character'' + the 'dj' character\n",
        "\n",
        "freqDist.pop('')\n",
        "freqDist.pop('dj')\n",
        "\n",
        "# Setting figure and ax into variables, for plotting\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "\n",
        "# Seaborn plotting using Pandas attributes + xtick rotation for ease of viewing:\n",
        "\n",
        "FreqDist_plot = sns.barplot(x=freqDist.index, y=freqDist.values, ax=ax)\n",
        "\n",
        "# Displaying the titles\n",
        "plt.xlabel('Words', fontsize=15)\n",
        "plt.ylabel('Frequency count', fontsize=15)\n",
        "plt.xticks(rotation=30)\n",
        "plt.title(\"Word frequency distribution\", fontsize=25)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RESULTS**: There were multiple company referencing code duplications in column ‘RIC’, so the dataset was grouped by this particular column and the text merged together. Unneccessary columns were also dropped in the script below, to make it sentiment analysis ready."
      ],
      "metadata": {
        "id": "DEwDn57158ic"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7e2746ac"
      },
      "outputs": [],
      "source": [
        "# Dropping unneccessary columns\n",
        "\n",
        "df3_dropped = df3.drop(['sdate'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94160f6c"
      },
      "outputs": [],
      "source": [
        "# Dropping unneccessary columns\n",
        "\n",
        "df3_dropped_final = df3_dropped.drop(['edate'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d39c9711"
      },
      "outputs": [],
      "source": [
        "# Visualizing the outcome\n",
        "\n",
        "df3_dropped_final.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eadce49f"
      },
      "outputs": [],
      "source": [
        "# Sorting the dataset by column RIC, as the enties are repeted\n",
        "\n",
        "headlines_sorted = df3_dropped_final.groupby(['RIC'])['Headlines'].apply(list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59ac84ef"
      },
      "outputs": [],
      "source": [
        "# Visualizing the sorded Headlines column by RIC column\n",
        "\n",
        "headlines_sorted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96591d00"
      },
      "outputs": [],
      "source": [
        "# Placing the result in a dataframe df\n",
        "\n",
        "df = pd.DataFrame(headlines_sorted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "c66149f1"
      },
      "outputs": [],
      "source": [
        "df # Done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1c6561d"
      },
      "outputs": [],
      "source": [
        "# Checking the dataframe type\n",
        "\n",
        "print(type(df['Headlines'] ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ebab087"
      },
      "outputs": [],
      "source": [
        "# Adjusting the headlines type to string, to make it sentiment analysis ready\n",
        "\n",
        "df['Headlines']  = df['Headlines'].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exporting the outcome into pickle, so we can use this dataset later, for further processing\n",
        "\n",
        "df.to_pickle(\"Cleaned headlines dataset\")"
      ],
      "metadata": {
        "id": "8st1nsutr47d"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}