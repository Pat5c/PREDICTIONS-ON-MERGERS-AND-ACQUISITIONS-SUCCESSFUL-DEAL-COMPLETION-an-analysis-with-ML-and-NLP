{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcdebfc5",
      "metadata": {
        "id": "dcdebfc5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np \n",
        "\n",
        "%pip install matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%pip install seaborn\n",
        "import seaborn as sns\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import regex as re\n",
        "from textblob import TextBlob\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import SVC \n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "pd.set_option('display.max_rows', 25)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the pickle file of the cleaned headlines dataset created in Appendix A, into a dataframe\n",
        "\n",
        "df = pd.read_pickle(\"Cleaned headlines dataset\")\n",
        "\n",
        "# View the DataFrame\n",
        "df"
      ],
      "metadata": {
        "id": "rt4nupszdD3h"
      },
      "id": "rt4nupszdD3h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mvx2wA1AJpCQ"
      },
      "id": "mvx2wA1AJpCQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "6a674f5c",
      "metadata": {
        "id": "6a674f5c"
      },
      "source": [
        "# Sentiment analysis using VADER\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0de918db",
      "metadata": {
        "id": "0de918db"
      },
      "outputs": [],
      "source": [
        "# Applying sentiment analysis using VADER on the cleaned Headlines column\n",
        " \n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "df['Scores'] = df['Headlines'].apply(analyzer.polarity_scores)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efd2b07a",
      "metadata": {
        "id": "efd2b07a"
      },
      "outputs": [],
      "source": [
        "# Converting the scores to string for further processing\n",
        "\n",
        "scores_str = df['Scores'].to_string()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5e6eff1",
      "metadata": {
        "scrolled": false,
        "id": "b5e6eff1"
      },
      "outputs": [],
      "source": [
        "# Splitting the results\n",
        "\n",
        "x = scores_str.split(\",\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b62324a6",
      "metadata": {
        "id": "b62324a6"
      },
      "outputs": [],
      "source": [
        "# Removing neg, neu, pos, compound from the dataset\n",
        "\n",
        "stopwords = ['neg', 'neu', 'pos', 'compound']\n",
        "\n",
        "new_words = [word for word in x if word not in stopwords]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "272c1fa7",
      "metadata": {
        "id": "272c1fa7"
      },
      "outputs": [],
      "source": [
        "# Placing the results into a dataframe\n",
        "\n",
        "df_1 = pd.DataFrame(new_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a077b2e7",
      "metadata": {
        "id": "a077b2e7"
      },
      "outputs": [],
      "source": [
        "# Clearing the column from words and leaving only numbers\n",
        "\n",
        "for col in df_1:\n",
        "    df_1[0] = [''.join(re.findall(\"\\d+\\.\\d+\", item)) for item in df_1[0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73db3f1e",
      "metadata": {
        "id": "73db3f1e"
      },
      "outputs": [],
      "source": [
        "# Adding a sentiment columm to clarify the numbers above\n",
        "\n",
        "L = ['Negative', 'Neutral', 'Positive']\n",
        "\n",
        "df_1['Sentiment'] =  (df_1.index % len(L)).map(dict(enumerate(L)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27bdcb45",
      "metadata": {
        "id": "27bdcb45"
      },
      "outputs": [],
      "source": [
        "# Adding an index\n",
        "\n",
        "df_1.reset_index(inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8ae0ba9",
      "metadata": {
        "id": "f8ae0ba9"
      },
      "outputs": [],
      "source": [
        "# Creating an ad hoc index, I can merge the 2 df together later\n",
        "\n",
        "l = [i for i in range(0,1057) for _ in range(3)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b7035f2",
      "metadata": {
        "id": "3b7035f2"
      },
      "outputs": [],
      "source": [
        "# Placing the index into a dataframe\n",
        "\n",
        "l = pd.DataFrame(l)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "502ba055",
      "metadata": {
        "id": "502ba055"
      },
      "outputs": [],
      "source": [
        "# Placing the index into index list object\n",
        "\n",
        "Index_list = l[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd8f7ee6",
      "metadata": {
        "id": "fd8f7ee6"
      },
      "outputs": [],
      "source": [
        "# Concatenating index and sentiment scores\n",
        "\n",
        "Concat_df = pd.concat([Index_list, (df_1.apply(pd.Series))], axis=1) # Working"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcc222da",
      "metadata": {
        "id": "bcc222da"
      },
      "outputs": [],
      "source": [
        "# Renaming columns\n",
        "\n",
        "Concate_New = Concat_df.rename({ 0 : 'Index'}, axis='columns')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23c3512c",
      "metadata": {
        "id": "23c3512c"
      },
      "outputs": [],
      "source": [
        "# Dropping the second index column\n",
        "\n",
        "Concate_dropped = Concate_New.drop(['index'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c48fd904",
      "metadata": {
        "id": "c48fd904"
      },
      "outputs": [],
      "source": [
        "# Renaming the columns\n",
        "\n",
        "Concate_dropped.columns = ['Index', 'Scores','Sentiment']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5564b7d",
      "metadata": {
        "id": "b5564b7d"
      },
      "outputs": [],
      "source": [
        "# Finalizing my sentiment df\n",
        "\n",
        "Sentiment_df = Concate_dropped.pivot(index='Index', columns='Sentiment', values= 'Scores')\n",
        "\n",
        "print(Sentiment_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22b8e77c",
      "metadata": {
        "id": "22b8e77c"
      },
      "outputs": [],
      "source": [
        "# Placing the sentiment scores into a dataframe\n",
        "\n",
        "Sentiment_df = pd.DataFrame(Sentiment_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cbe7a5c",
      "metadata": {
        "id": "7cbe7a5c"
      },
      "outputs": [],
      "source": [
        "# Filtering sentiment scores for the columns needed\n",
        "\n",
        "Sentiment_df = Sentiment_df.iloc[1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b95a1df5",
      "metadata": {
        "id": "b95a1df5"
      },
      "outputs": [],
      "source": [
        "# Counting the scores for better visualization\n",
        "\n",
        "Counted = Sentiment_df.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56270309",
      "metadata": {
        "scrolled": true,
        "id": "56270309"
      },
      "outputs": [],
      "source": [
        "# Setting the correct column type\n",
        "\n",
        "Sentiment_df[\"Negative\"] = Sentiment_df[\"Negative\"].astype(str).astype(float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3edbc82f",
      "metadata": {
        "scrolled": true,
        "id": "3edbc82f"
      },
      "outputs": [],
      "source": [
        "Sentiment_df[\"Neutral\"] = Sentiment_df[\"Neutral\"].astype(str).astype(float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e7a2624",
      "metadata": {
        "scrolled": true,
        "id": "7e7a2624"
      },
      "outputs": [],
      "source": [
        "Sentiment_df[\"Positive\"] = Sentiment_df[\"Positive\"].astype(str).astype(float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28a4fdc3",
      "metadata": {
        "id": "28a4fdc3"
      },
      "outputs": [],
      "source": [
        "# Plotting a bar chart of the sentiment distribution\n",
        "\n",
        "Sentiment_melted = Sentiment_df.melt(var_name='cols', value_name='vals')\n",
        "\n",
        "Sentiment_melted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea4aa72b",
      "metadata": {
        "id": "ea4aa72b"
      },
      "outputs": [],
      "source": [
        "# Adjusting the column type\n",
        "\n",
        "Sentiment_melted[\"vals\"] = Sentiment_melted[\"vals\"].astype(str).astype(float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2d8d2a6",
      "metadata": {
        "id": "e2d8d2a6"
      },
      "outputs": [],
      "source": [
        "# Dropping zero's\n",
        "\n",
        "Sentiment_melted_cleaned = Sentiment_melted[Sentiment_melted.vals != 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff31a9cb",
      "metadata": {
        "id": "ff31a9cb"
      },
      "outputs": [],
      "source": [
        "# Plotting the sentiment scores distribution\n",
        "\n",
        "g = sns.barplot(x=Sentiment_melted_cleaned.index, y=\"vals\", hue='cols', data=Sentiment_melted_cleaned, linewidth = 0.1)\n",
        "plt.legend(title = 'Sentiment', bbox_to_anchor = (1, 1))\n",
        "plt.xlabel('Sentiment', fontsize=10)\n",
        "plt.ylabel('Sentiment Scores', fontsize=10)\n",
        "plt.title('Sentiment scores distribution', fontsize=20)\n",
        "g.set(xticklabels=[]) \n",
        "g.tick_params(bottom=False)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de8c59a1",
      "metadata": {
        "id": "de8c59a1"
      },
      "outputs": [],
      "source": [
        "# Getting a count of the sentiment values\n",
        "\n",
        "Data = Sentiment_melted_cleaned['cols'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c795fa8",
      "metadata": {
        "id": "8c795fa8"
      },
      "outputs": [],
      "source": [
        "# Placing the result into a dataframe\n",
        "\n",
        "Data = pd.DataFrame(Data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9309b1a",
      "metadata": {
        "id": "d9309b1a"
      },
      "outputs": [],
      "source": [
        "# Plotting a bar graph of the sentiment counts\n",
        "\n",
        "g = sns.barplot(x=Data.index, y='cols', hue='cols', data=Data, linewidth = 0.1)\n",
        "g.get_legend().remove()\n",
        "plt.ylabel('Value counts', fontsize=10)\n",
        "plt.title('Sentiment value counts', fontsize=20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fcd1a51",
      "metadata": {
        "id": "4fcd1a51"
      },
      "outputs": [],
      "source": [
        "# Dropping the scores column from the initial dataframe for further processing\n",
        "\n",
        "df_dropped = df.drop(['Scores'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd1a3699",
      "metadata": {
        "id": "bd1a3699"
      },
      "outputs": [],
      "source": [
        "# Resetting the index\n",
        "\n",
        "df_dropped.reset_index(inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e255ecb",
      "metadata": {
        "id": "5e255ecb"
      },
      "outputs": [],
      "source": [
        "# Merging this newly created Sentiment table to the initial dataset\n",
        "\n",
        "Headline_Sentiment = pd.concat([df_dropped, Sentiment_df], axis=1)\n",
        "Headline_Sentiment.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad16fd77",
      "metadata": {
        "id": "ad16fd77"
      },
      "outputs": [],
      "source": [
        "# Getting the VADER coumpound scores, which will be utilized later when training the Ml algorithms\n",
        "\n",
        "scores = []\n",
        "\n",
        "# Declaring the variable for the compound scores\n",
        "\n",
        "compound_list = []\n",
        "\n",
        "for i in range(df['Headlines'].shape[0]):\n",
        "\n",
        "# Creating the scores and appending the results into a separate dataframa columns\n",
        "\n",
        "    compound = analyzer.polarity_scores(df['Headlines'][i])[\"compound\"]\n",
        "\n",
        "    scores.append({\"Compound\": compound})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "163b4102",
      "metadata": {
        "id": "163b4102"
      },
      "outputs": [],
      "source": [
        "# Results of the above:\n",
        "\n",
        "Compound_score = pd.DataFrame.from_dict(scores)\n",
        "Compound_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65a429a4",
      "metadata": {
        "id": "65a429a4"
      },
      "outputs": [],
      "source": [
        "# This is the sentiment dataset, inclusive of the compound scores\n",
        "\n",
        "df_comp = Headline_Sentiment.join(Compound_score)\n",
        "df_comp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping Negative column as I will not need it\n",
        "\n",
        "Headlines_Vader = df_comp.drop(['Negative'], axis=1)"
      ],
      "metadata": {
        "id": "r8rRAfHJq6IX"
      },
      "id": "r8rRAfHJq6IX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping Neutral column as I will not need it\n",
        "\n",
        "Headlines_Vader_1 = Headlines_Vader.drop(['Neutral'], axis=1)"
      ],
      "metadata": {
        "id": "LA6l-VPKreeU"
      },
      "id": "LA6l-VPKreeU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping Positive column as I will not need it\n",
        "\n",
        "Headlines_Vader_ok = Headlines_Vader_1.drop(['Positive'], axis=1)"
      ],
      "metadata": {
        "id": "yCSXujZWreqv"
      },
      "id": "yCSXujZWreqv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final VADER sentiment Compound scores dataset, ready for further analysis\n",
        "\n",
        "Headlines_Vader_ok"
      ],
      "metadata": {
        "id": "386JCmX6rvo8"
      },
      "id": "386JCmX6rvo8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1c512e9",
      "metadata": {
        "id": "a1c512e9"
      },
      "outputs": [],
      "source": [
        "# Plotting the Compound score fo further visualization\n",
        "\n",
        "Compound_score.plot(kind='kde')\n",
        "plt.xlabel('Compound score', fontsize=10)\n",
        "plt.ylabel('Compound density', fontsize=10)\n",
        "plt.title('VADER Compound scores', fontsize=20)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52094f0a",
      "metadata": {
        "id": "52094f0a"
      },
      "source": [
        "# Sentiment analysis using TEXTBLOB"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjusting the Headlines column type\n",
        "\n",
        "df['Headlines']  = df['Headlines'].astype(str)"
      ],
      "metadata": {
        "id": "REebZfXTiGVx"
      },
      "id": "REebZfXTiGVx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46d0313e",
      "metadata": {
        "id": "46d0313e"
      },
      "outputs": [],
      "source": [
        "# Calcolating TextBlob sentiment scores. Placing the results into a dataframe\n",
        "\n",
        "df['Polarity'] = np.nan\n",
        "df['Subjectivity'] = np.nan\n",
        "\n",
        "pd.options.mode.chained_assignment = None\n",
        "\n",
        "for idx, Headlines in enumerate(df['Headlines'].values):  # for each row in our df dataFrame\n",
        "        if Headlines:\n",
        "            sentA = TextBlob(Headlines) # pass the text only article to TextBlob to analyse\n",
        "            df['Polarity'].iloc[idx] = sentA.sentiment.polarity # write sentiment polarity back to df\n",
        "            df['Subjectivity'].iloc[idx] = sentA.sentiment.subjectivity # write sentiment subjectivity score back to df\n",
        "\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70038c19",
      "metadata": {
        "id": "70038c19"
      },
      "outputs": [],
      "source": [
        "# Removing the Scores column\n",
        "\n",
        "df.drop(['Scores'], axis=1, inplace = True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "662bd365",
      "metadata": {
        "id": "662bd365"
      },
      "outputs": [],
      "source": [
        "# Plotting the TextBlob Polarity and Subjectivity scores\n",
        "\n",
        "df.plot(kind='kde')\n",
        "plt.xlabel('Scores', fontsize=10)\n",
        "plt.ylabel('Density', fontsize=10)\n",
        "plt.title('TEXTBLOB Polarity and Subjectivity scores', fontsize=20)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping the unneccessary Headlines column\n",
        "\n",
        "df.drop(['Headlines'], axis=1, inplace = True)"
      ],
      "metadata": {
        "id": "LRtGT9WWGYxf"
      },
      "id": "LRtGT9WWGYxf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.reset_index(inplace = True)\n",
        "df"
      ],
      "metadata": {
        "id": "HIlX49ShVNT4"
      },
      "id": "HIlX49ShVNT4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# APPROACH A: Share price with sentiment scores"
      ],
      "metadata": {
        "id": "vfW5p6yldFWZ"
      },
      "id": "vfW5p6yldFWZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VADER"
      ],
      "metadata": {
        "id": "XXUx18y8Ra2P"
      },
      "id": "XXUx18y8Ra2P"
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading the pickle file from Appendix A (full financial dataset)\n",
        "\n",
        "num_RIC_features = pd.read_pickle(\"Merged numerical financial dataset\")\n",
        "\n",
        "#view DataFrame\n",
        "num_RIC_features"
      ],
      "metadata": {
        "id": "CzOhAt5PdD86"
      },
      "id": "CzOhAt5PdD86",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Renaming column Instrument to RIC, for further processing\n",
        "\n",
        "num_RIC_features.rename(columns = { 'Instrument' :'RIC'}, inplace = True)"
      ],
      "metadata": {
        "id": "MGEsOGpYdEAM"
      },
      "id": "MGEsOGpYdEAM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing duplicated in column RIC\n",
        "\n",
        "num_RIC_features.drop_duplicates(subset=['RIC'])"
      ],
      "metadata": {
        "id": "IBG9sQu4dEDG"
      },
      "id": "IBG9sQu4dEDG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merging the VADER scores dataset together with the financial datasite on column RIC\n",
        "\n",
        "Vader_num_df = num_RIC_features.merge(Headlines_Vader_ok[['Compound', 'RIC']], on = 'RIC')"
      ],
      "metadata": {
        "id": "Dlv5ePRidEFv"
      },
      "id": "Dlv5ePRidEFv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final dataset merged on the single values of the RIC column. Is a 1883 rows × 24 columns dataframe\n",
        "\n",
        "Vader_num_df"
      ],
      "metadata": {
        "id": "m3dgUn7QdEJG"
      },
      "id": "m3dgUn7QdEJG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing share price dataset\n",
        "# Remove all columns between column index 1 to 3\n",
        "\n",
        "Vader_share_price = Vader_num_df.drop(Vader_num_df.iloc[:, 2:11], axis=1)\n"
      ],
      "metadata": {
        "id": "8CnIRmxUleBs"
      },
      "id": "8CnIRmxUleBs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping uneccessary columns\n",
        "\n",
        "Vader_share_price.drop(Vader_share_price.iloc[:, 2:5], inplace = True, axis=1)"
      ],
      "metadata": {
        "id": "zRGem4xILLk5"
      },
      "id": "zRGem4xILLk5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping uneccessary columns\n",
        "\n",
        "Vader_share_price.drop(Vader_share_price.iloc[:, 5:10], inplace = True, axis=1)"
      ],
      "metadata": {
        "id": "jmxUWqk1LgGI"
      },
      "id": "jmxUWqk1LgGI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping uneccessary columns\n",
        "\n",
        "Vader_share_price.drop(['Index','Current Ratio','Cash & Cash Equivalents - Total'], inplace = True, axis=1)\n"
      ],
      "metadata": {
        "id": "0_9WTOClJ--4"
      },
      "id": "0_9WTOClJ--4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping uneccessary columns\n",
        "\n",
        "Vader_share_price"
      ],
      "metadata": {
        "id": "gaj5PkI3leK7"
      },
      "id": "gaj5PkI3leK7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Applying ML on Share price dataset with VADER compound scores"
      ],
      "metadata": {
        "id": "T_O_3-7tNRj5"
      },
      "id": "T_O_3-7tNRj5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8_q3RFhgPpW"
      },
      "outputs": [],
      "source": [
        "# Choosing a subset of the data to split in between train and test:\n",
        "\n",
        "X = Vader_share_price\n",
        "y = Vader_share_price['Target/Non-Target']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size=0.3, random_state=11)"
      ],
      "id": "P8_q3RFhgPpW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BL57CzA5jfje"
      },
      "outputs": [],
      "source": [
        "# BASELINE MODEL logistic regression\n",
        "\n",
        "# Baseline performance 2: Logistic regression classifier\n",
        "\n",
        "lr = LogisticRegression()\n",
        "\n",
        "# Applying the model to the training data:\n",
        "\n",
        "lr.fit(X_train,y_train)\n",
        "\n",
        "\n",
        "# Predict the test model:\n",
        "labels_lr = lr.predict(X_test)\n"
      ],
      "id": "BL57CzA5jfje"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMx7qGxijfsc"
      },
      "outputs": [],
      "source": [
        "# Let's evaluate the results with accuracy:\n",
        "\n",
        "print('Logistic Regression Test Accuracy:', accuracy_score(y_test, labels_lr))\n",
        "print('Logistic Regression Train Accuracy:', accuracy_score(y_train, lr.predict(X_train)))\n",
        "\n",
        "# Recall - but also precision, f1-score and support:\n",
        "\n",
        "print(classification_report(y_test, labels_lr))\n",
        "print(classification_report(y_train, lr.predict(X_train)))\n",
        "\n",
        "# Confusion matrix:\n",
        "\n",
        "mat_lr = confusion_matrix(y_test,labels_lr)\n",
        "sns.heatmap(mat_lr, square=True, annot=True, fmt=\"d\", cbar=False,\n",
        "           xticklabels=['Non-Target', 'Target'], yticklabels=['Non-Target', 'Target'])\n",
        "\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('Label')"
      ],
      "id": "NMx7qGxijfsc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5XxexN5DbPoT"
      },
      "outputs": [],
      "source": [
        "# Precision\n",
        "\n",
        "precision = precision_score(y_test, labels_lr, average=None)\n",
        "print(precision)\n",
        "\n",
        "# Recall\n",
        "\n",
        "recall = recall_score(y_test, labels_lr, average=None)\n",
        "print(recall)\n",
        "\n",
        "# F-score\n",
        "f_score = f1_score(y_test, labels_lr, average=None)\n",
        "print(f_score)"
      ],
      "id": "5XxexN5DbPoT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGZ8tfZRjfyU"
      },
      "outputs": [],
      "source": [
        "# Random Forest on feature selected dataset with Target\n",
        "\n",
        "# Create regressor object\n",
        "    \n",
        "regressor = RandomForestRegressor(n_estimators = 100, random_state = 0)\n",
        "  \n",
        "# Applying the model to the training data:\n",
        "\n",
        "regressor.fit(X_train,y_train)\n",
        "\n",
        "# Predict the test model:\n",
        "\n",
        "labels_regressor = regressor.predict(X_test)\n",
        "\n"
      ],
      "id": "NGZ8tfZRjfyU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0m1ES7kHpAeR"
      },
      "outputs": [],
      "source": [
        "# Let's evalueate the results with accuracy:\n",
        "\n",
        "print('Random Forest Test Accuracy:', accuracy_score(y_test, labels_regressor))\n",
        "print('Random Forest Train Accuracy:', accuracy_score(y_train, regressor.predict(X_train)))\n",
        "\n",
        "# Recall - but also precision, f1-score and support:\n",
        "\n",
        "print(classification_report(y_test, labels_regressor))\n",
        "print(classification_report(y_train, regressor.predict(X_train)))\n",
        "\n",
        "# Confusion matrix:\n",
        "\n",
        "mat_regressor = confusion_matrix(y_test,labels_regressor)\n",
        "sns.heatmap(mat_regressor, square=True, annot=True, fmt=\"d\", cbar=False,\n",
        "           xticklabels=['Non-Target', 'Target'], yticklabels=['Non-Target', 'Target'])\n",
        "\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('Label')"
      ],
      "id": "0m1ES7kHpAeR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIB_RLJ1bmpg"
      },
      "outputs": [],
      "source": [
        "# Precision\n",
        "\n",
        "precision = precision_score(y_test, labels_regressor, average=None)\n",
        "print(precision)\n",
        "\n",
        "# Recall\n",
        "\n",
        "recall = recall_score(y_test, labels_regressor, average=None)\n",
        "print(recall)\n",
        "\n",
        "# F-score\n",
        "f_score = f1_score(y_test, labels_regressor, average=None)\n",
        "print(f_score)"
      ],
      "id": "HIB_RLJ1bmpg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xgnhh8igpAh7"
      },
      "outputs": [],
      "source": [
        "# Neural Network NN\n",
        "# Building the classifier\n",
        "\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(13,13,13),max_iter=500)\n",
        "\n",
        "# Applying the model to the training data:\n",
        "\n",
        "mlp.fit(X_train,y_train)\n",
        "\n",
        "# Predict the test model:\n",
        "\n",
        "labels_mlp = mlp.predict(X_test)\n"
      ],
      "id": "Xgnhh8igpAh7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ai4NAGhpAl8"
      },
      "outputs": [],
      "source": [
        "# Let's evalueate the results with accuracy:\n",
        "\n",
        "print('NN Test Accuracy:', accuracy_score(y_test, labels_mlp))\n",
        "print('NN Train Accuracy:', accuracy_score(y_train, mlp.predict(X_train)))\n",
        "\n",
        "# Recall - but also precision, f1-score and support:\n",
        "\n",
        "print(classification_report(y_test, labels_mlp))\n",
        "print(classification_report(y_train, mlp.predict(X_train)))\n",
        "\n",
        "# Confusion matrix:\n",
        "\n",
        "mat_mlp = confusion_matrix(y_test,labels_mlp)\n",
        "sns.heatmap(mat_mlp, square=True, annot=True, fmt=\"d\", cbar=False,\n",
        "           xticklabels=['Non-Target', 'Target'], yticklabels=['Non-Target', 'Target'])\n",
        "\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('Label')\n"
      ],
      "id": "7ai4NAGhpAl8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dsUR1wNpAuw"
      },
      "outputs": [],
      "source": [
        "# Applying nested cross-validation check:\n",
        "scores_mlp = cross_val_score(mlp, X, y, cv=10, scoring='accuracy')\n",
        "print(scores_mlp)\n",
        "print(\"%0.4f accuracy with a standard deviation of %0.4f\" % (scores_mlp.mean(), scores_mlp.std()))"
      ],
      "id": "0dsUR1wNpAuw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4npIi0fb8tg"
      },
      "outputs": [],
      "source": [
        "# Precision\n",
        "\n",
        "precision = precision_score(y_test, labels_mlp, average=None)\n",
        "print(precision)\n",
        "\n",
        "# Recall\n",
        "\n",
        "recall = recall_score(y_test, labels_mlp, average=None)\n",
        "print(recall)\n",
        "\n",
        "# F-score\n",
        "f_score = f1_score(y_test, labels_mlp, average=None)\n",
        "print(f_score)"
      ],
      "id": "b4npIi0fb8tg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_L4HKK0gp-CH"
      },
      "outputs": [],
      "source": [
        "# Decision Tree\n",
        "\n",
        "dt = DecisionTreeClassifier(criterion='entropy')\n",
        "\n",
        "# Applying the model to the training data:\n",
        "\n",
        "dt.fit(X_train,y_train)\n",
        "\n",
        "# Predict the test model:\n",
        "\n",
        "labels_dt = dt.predict(X_test)\n"
      ],
      "id": "_L4HKK0gp-CH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8lfjuqsp-K1"
      },
      "outputs": [],
      "source": [
        "# Let's evalueate the results with accuracy:\n",
        "\n",
        "print('Decision Tree Test Accuracy:', accuracy_score(y_test, labels_dt))\n",
        "print('Decision Tree Train Accuracy:', accuracy_score(y_train, dt.predict(X_train)))\n",
        "\n",
        "# Recall - but also precision, f1-score and support:\n",
        "\n",
        "print(classification_report(y_test, labels_dt))\n",
        "print(classification_report(y_train, dt.predict(X_train)))\n",
        "\n",
        "# Confusion matrix:\n",
        "\n",
        "mat_dt = confusion_matrix(y_test,labels_dt)\n",
        "sns.heatmap(mat_dt, square=True, annot=True, fmt=\"d\", cbar=False,\n",
        "           xticklabels=['Non-Target', 'Target'], yticklabels=['Non-Target', 'Target'])\n",
        "\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('Label')\n"
      ],
      "id": "B8lfjuqsp-K1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W17CF8YBdafO"
      },
      "outputs": [],
      "source": [
        "# Precision\n",
        "\n",
        "precision = precision_score(y_test, labels_dt, average=None)\n",
        "print(precision)\n",
        "\n",
        "# Recall\n",
        "\n",
        "recall = recall_score(y_test, labels_dt, average=None)\n",
        "print(recall)\n",
        "\n",
        "# F-score\n",
        "f_score = f1_score(y_test, labels_dt, average=None)\n",
        "print(f_score)"
      ],
      "id": "W17CF8YBdafO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRP9pRMiqzrK"
      },
      "outputs": [],
      "source": [
        "# Support Vector Machine (SVM)\n",
        "\n",
        "# Building the linear Support Vector Machine Classifier\n",
        "\n",
        "Svm = LinearSVC(dual = False, random_state = 0, penalty = 'l1',tol = 1e-5)\n",
        "\n",
        "Svm.fit(X_train,y_train) \n",
        "\n",
        "# Predict the test model:\n",
        "\n",
        "labels_svm = Svm.predict(X_test)\n"
      ],
      "id": "tRP9pRMiqzrK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMGAd9M2p-S4"
      },
      "outputs": [],
      "source": [
        "# Let's evalueate the results with accuracy:\n",
        "\n",
        "print('SVM Test Accuracy:', accuracy_score(y_test, labels_svm))\n",
        "print('SVM Train Accuracy:', accuracy_score(y_train, Svm.predict(X_train)))\n",
        "\n",
        "# Recall - but also precision, f1-score and support:\n",
        "\n",
        "print(classification_report(y_test, labels_svm))\n",
        "print(classification_report(y_train, Svm.predict(X_train)))\n",
        "\n",
        "# Confusion matrix:\n",
        "\n",
        "mat_svm = confusion_matrix(y_test,labels_svm)\n",
        "sns.heatmap(mat_svm, square=True, annot=True, fmt=\"d\", cbar=False,\n",
        "           xticklabels=['Non-Terget', 'Terget'], yticklabels=['Non-Terget', 'Terget'])\n",
        "\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('Label')\n"
      ],
      "id": "VMGAd9M2p-S4"
    },
    {
      "cell_type": "code",
      "source": [
        "# Precision\n",
        "\n",
        "precision = precision_score(y_test, labels_svm, average=None)\n",
        "print(precision)\n",
        "\n",
        "# Recall\n",
        "\n",
        "recall = recall_score(y_test, labels_svm, average=None)\n",
        "print(recall)\n",
        "\n",
        "# F-score\n",
        "f_score = f1_score(y_test, labels_svm, average=None)\n",
        "print(f_score)"
      ],
      "metadata": {
        "id": "UQAVKi8zQa3L"
      },
      "id": "UQAVKi8zQa3L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TEXTBLOB"
      ],
      "metadata": {
        "id": "BdgHvAL7Rfo4"
      },
      "id": "BdgHvAL7Rfo4"
    },
    {
      "cell_type": "code",
      "source": [
        "# Merging the TEXTBLOB scores dataset together with the financial datasite on column RIC\n",
        "\n",
        "Textblob_num_df = num_RIC_features.merge(df[['Polarity','Subjectivity','RIC']], on = 'RIC')"
      ],
      "metadata": {
        "id": "Xf4adj6wQf2Z"
      },
      "id": "Xf4adj6wQf2Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Textblob_num_df"
      ],
      "metadata": {
        "id": "QqojcPFIRjzi"
      },
      "id": "QqojcPFIRjzi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing share price dataset\n",
        "# Remove all columns between column index 1 to 3\n",
        "\n",
        "Textblob_share_price = Textblob_num_df.drop(Textblob_num_df.iloc[:, 2:11], axis=1)\n"
      ],
      "metadata": {
        "id": "t8jKy1wXWTHo"
      },
      "execution_count": null,
      "outputs": [],
      "id": "t8jKy1wXWTHo"
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping uneccessary columns\n",
        "\n",
        "Textblob_share_price.drop(Textblob_share_price.iloc[:, 2:5], inplace = True, axis=1)"
      ],
      "metadata": {
        "id": "q_WKxgM_WTH3"
      },
      "execution_count": null,
      "outputs": [],
      "id": "q_WKxgM_WTH3"
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping uneccessary columns\n",
        "\n",
        "Textblob_share_price.drop(Textblob_share_price.iloc[:, 5:10], inplace = True, axis=1)"
      ],
      "metadata": {
        "id": "OVy27nO7WTH3"
      },
      "execution_count": null,
      "outputs": [],
      "id": "OVy27nO7WTH3"
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping uneccessary columns\n",
        "\n",
        "Textblob_share_price.drop(['Index','Current Ratio','Cash & Cash Equivalents - Total'], inplace = True, axis=1)\n"
      ],
      "metadata": {
        "id": "8GexedVqWTH4"
      },
      "execution_count": null,
      "outputs": [],
      "id": "8GexedVqWTH4"
    },
    {
      "cell_type": "code",
      "source": [
        "Textblob_share_price"
      ],
      "metadata": {
        "id": "4X6I0_oJXuJ1"
      },
      "id": "4X6I0_oJXuJ1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Applying ML on Share price dataset with TEXTBLOB Polarity and Subjectivity scores"
      ],
      "metadata": {
        "id": "UNl9hrCVYLaI"
      },
      "id": "UNl9hrCVYLaI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yakxZCmYLaK"
      },
      "outputs": [],
      "source": [
        "# Choosing a subset of the data to split in between train and test:\n",
        "\n",
        "X = Textblob_share_price\n",
        "y = Textblob_share_price['Target/Non-Target']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size=0.3, random_state=11)"
      ],
      "id": "7yakxZCmYLaK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXDOSz7PYLaK"
      },
      "outputs": [],
      "source": [
        "# BASELINE MODEL logistic regression\n",
        "\n",
        "# Baseline performance 2: Logistic regression classifier\n",
        "\n",
        "lr = LogisticRegression()\n",
        "\n",
        "# Applying the model to the training data:\n",
        "\n",
        "lr.fit(X_train,y_train)\n",
        "\n",
        "\n",
        "# Predict the test model:\n",
        "labels_lr = lr.predict(X_test)\n"
      ],
      "id": "XXDOSz7PYLaK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljmEih48YLaK"
      },
      "outputs": [],
      "source": [
        "# Let's evaluate the results with accuracy:\n",
        "\n",
        "print('Logistic Regression Test Accuracy:', accuracy_score(y_test, labels_lr))\n",
        "print('Logistic Regression Train Accuracy:', accuracy_score(y_train, lr.predict(X_train)))\n",
        "\n",
        "# Recall - but also precision, f1-score and support:\n",
        "\n",
        "print(classification_report(y_test, labels_lr))\n",
        "print(classification_report(y_train, lr.predict(X_train)))\n",
        "\n",
        "# Confusion matrix:\n",
        "\n",
        "mat_lr = confusion_matrix(y_test,labels_lr)\n",
        "sns.heatmap(mat_lr, square=True, annot=True, fmt=\"d\", cbar=False,\n",
        "           xticklabels=['Non-Target', 'Target'], yticklabels=['Non-Target', 'Target'])\n",
        "\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('Label')"
      ],
      "id": "ljmEih48YLaK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3is7Qo0YLaL"
      },
      "outputs": [],
      "source": [
        "# Precision\n",
        "\n",
        "precision = precision_score(y_test, labels_lr, average=None)\n",
        "print(precision)\n",
        "\n",
        "# Recall\n",
        "\n",
        "recall = recall_score(y_test, labels_lr, average=None)\n",
        "print(recall)\n",
        "\n",
        "# F-score\n",
        "f_score = f1_score(y_test, labels_lr, average=None)\n",
        "print(f_score)"
      ],
      "id": "F3is7Qo0YLaL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehfi8hbkYLaM"
      },
      "outputs": [],
      "source": [
        "# Random Forest on feature selected dataset with Target\n",
        "\n",
        "# Create regressor object\n",
        "    \n",
        "regressor = RandomForestRegressor(n_estimators = 100, random_state = 0)\n",
        "  \n",
        "# Applying the model to the training data:\n",
        "\n",
        "regressor.fit(X_train,y_train)\n",
        "\n",
        "# Predict the test model:\n",
        "\n",
        "labels_regressor = regressor.predict(X_test)\n",
        "\n"
      ],
      "id": "ehfi8hbkYLaM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPG-PQMhYLaM"
      },
      "outputs": [],
      "source": [
        "# Let's evalueate the results with accuracy:\n",
        "\n",
        "print('Random Forest Test Accuracy:', accuracy_score(y_test, labels_regressor))\n",
        "print('Random Forest Train Accuracy:', accuracy_score(y_train, regressor.predict(X_train)))\n",
        "\n",
        "# Recall - but also precision, f1-score and support:\n",
        "\n",
        "print(classification_report(y_test, labels_regressor))\n",
        "print(classification_report(y_train, regressor.predict(X_train)))\n",
        "\n",
        "# Confusion matrix:\n",
        "\n",
        "mat_regressor = confusion_matrix(y_test,labels_regressor)\n",
        "sns.heatmap(mat_regressor, square=True, annot=True, fmt=\"d\", cbar=False,\n",
        "           xticklabels=['Non-Target', 'Target'], yticklabels=['Non-Target', 'Target'])\n",
        "\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('Label')"
      ],
      "id": "lPG-PQMhYLaM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYzJ88SxYLaM"
      },
      "outputs": [],
      "source": [
        "# Precision\n",
        "\n",
        "precision = precision_score(y_test, labels_regressor, average=None)\n",
        "print(precision)\n",
        "\n",
        "# Recall\n",
        "\n",
        "recall = recall_score(y_test, labels_regressor, average=None)\n",
        "print(recall)\n",
        "\n",
        "# F-score\n",
        "f_score = f1_score(y_test, labels_regressor, average=None)\n",
        "print(f_score)"
      ],
      "id": "nYzJ88SxYLaM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LoeUvg0EYLaN"
      },
      "outputs": [],
      "source": [
        "# Neural Network NN\n",
        "# Building the classifier\n",
        "\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(13,13,13),max_iter=500)\n",
        "\n",
        "# Applying the model to the training data:\n",
        "\n",
        "mlp.fit(X_train,y_train)\n",
        "\n",
        "# Predict the test model:\n",
        "\n",
        "labels_mlp = mlp.predict(X_test)\n"
      ],
      "id": "LoeUvg0EYLaN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05DRUWaUYLaN"
      },
      "outputs": [],
      "source": [
        "# Let's evalueate the results with accuracy:\n",
        "\n",
        "print('NN Test Accuracy:', accuracy_score(y_test, labels_mlp))\n",
        "print('NN Train Accuracy:', accuracy_score(y_train, mlp.predict(X_train)))\n",
        "\n",
        "# Recall - but also precision, f1-score and support:\n",
        "\n",
        "print(classification_report(y_test, labels_mlp))\n",
        "print(classification_report(y_train, mlp.predict(X_train)))\n",
        "\n",
        "# Confusion matrix:\n",
        "\n",
        "mat_mlp = confusion_matrix(y_test,labels_mlp)\n",
        "sns.heatmap(mat_mlp, square=True, annot=True, fmt=\"d\", cbar=False,\n",
        "           xticklabels=['Non-Target', 'Target'], yticklabels=['Non-Target', 'Target'])\n",
        "\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('Label')\n"
      ],
      "id": "05DRUWaUYLaN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5Oq3sGAYLaN"
      },
      "outputs": [],
      "source": [
        "# Applying nested cross-validation check:\n",
        "scores_mlp = cross_val_score(mlp, X, y, cv=10, scoring='accuracy')\n",
        "print(scores_mlp)\n",
        "print(\"%0.4f accuracy with a standard deviation of %0.4f\" % (scores_mlp.mean(), scores_mlp.std()))"
      ],
      "id": "x5Oq3sGAYLaN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nsthCPzYLaO"
      },
      "outputs": [],
      "source": [
        "# Precision\n",
        "\n",
        "precision = precision_score(y_test, labels_mlp, average=None)\n",
        "print(precision)\n",
        "\n",
        "# Recall\n",
        "\n",
        "recall = recall_score(y_test, labels_mlp, average=None)\n",
        "print(recall)\n",
        "\n",
        "# F-score\n",
        "f_score = f1_score(y_test, labels_mlp, average=None)\n",
        "print(f_score)"
      ],
      "id": "7nsthCPzYLaO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSYEwAeUYLaO"
      },
      "outputs": [],
      "source": [
        "# Decision Tree\n",
        "\n",
        "dt = DecisionTreeClassifier(criterion='entropy')\n",
        "\n",
        "# Applying the model to the training data:\n",
        "\n",
        "dt.fit(X_train,y_train)\n",
        "\n",
        "# Predict the test model:\n",
        "\n",
        "labels_dt = dt.predict(X_test)\n"
      ],
      "id": "DSYEwAeUYLaO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Li1VZGEYLaO"
      },
      "outputs": [],
      "source": [
        "# Let's evalueate the results with accuracy:\n",
        "\n",
        "print('Decision Tree Test Accuracy:', accuracy_score(y_test, labels_dt))\n",
        "print('Decision Tree Train Accuracy:', accuracy_score(y_train, dt.predict(X_train)))\n",
        "\n",
        "# Recall - but also precision, f1-score and support:\n",
        "\n",
        "print(classification_report(y_test, labels_dt))\n",
        "print(classification_report(y_train, dt.predict(X_train)))\n",
        "\n",
        "# Confusion matrix:\n",
        "\n",
        "mat_dt = confusion_matrix(y_test,labels_dt)\n",
        "sns.heatmap(mat_dt, square=True, annot=True, fmt=\"d\", cbar=False,\n",
        "           xticklabels=['Non-Target', 'Target'], yticklabels=['Non-Target', 'Target'])\n",
        "\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('Label')\n"
      ],
      "id": "1Li1VZGEYLaO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_nvLGDDYLaO"
      },
      "outputs": [],
      "source": [
        "# Precision\n",
        "\n",
        "precision = precision_score(y_test, labels_dt, average=None)\n",
        "print(precision)\n",
        "\n",
        "# Recall\n",
        "\n",
        "recall = recall_score(y_test, labels_dt, average=None)\n",
        "print(recall)\n",
        "\n",
        "# F-score\n",
        "f_score = f1_score(y_test, labels_dt, average=None)\n",
        "print(f_score)"
      ],
      "id": "Z_nvLGDDYLaO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ga_GcZgYLaP"
      },
      "outputs": [],
      "source": [
        "# Support Vector Machine (SVM)\n",
        "\n",
        "# Building the linear Support Vector Machine Classifier\n",
        "\n",
        "Svm = LinearSVC(dual = False, random_state = 0, penalty = 'l1',tol = 1e-5)\n",
        "\n",
        "Svm.fit(X_train,y_train) \n",
        "\n",
        "# Predict the test model:\n",
        "\n",
        "labels_svm = Svm.predict(X_test)\n"
      ],
      "id": "5ga_GcZgYLaP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJr7fs-vYLaP"
      },
      "outputs": [],
      "source": [
        "# Let's evalueate the results with accuracy:\n",
        "\n",
        "print('SVM Test Accuracy:', accuracy_score(y_test, labels_svm))\n",
        "print('SVM Train Accuracy:', accuracy_score(y_train, Svm.predict(X_train)))\n",
        "\n",
        "# Recall - but also precision, f1-score and support:\n",
        "\n",
        "print(classification_report(y_test, labels_svm))\n",
        "print(classification_report(y_train, Svm.predict(X_train)))\n",
        "\n",
        "# Confusion matrix:\n",
        "\n",
        "mat_svm = confusion_matrix(y_test,labels_svm)\n",
        "sns.heatmap(mat_svm, square=True, annot=True, fmt=\"d\", cbar=False,\n",
        "           xticklabels=['Non-Terget', 'Terget'], yticklabels=['Non-Terget', 'Terget'])\n",
        "\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('Label')\n"
      ],
      "id": "IJr7fs-vYLaP"
    },
    {
      "cell_type": "code",
      "source": [
        "# Precision\n",
        "\n",
        "precision = precision_score(y_test, labels_svm, average=None)\n",
        "print(precision)\n",
        "\n",
        "# Recall\n",
        "\n",
        "recall = recall_score(y_test, labels_svm, average=None)\n",
        "print(recall)\n",
        "\n",
        "# F-score\n",
        "f_score = f1_score(y_test, labels_svm, average=None)\n",
        "print(f_score)"
      ],
      "metadata": {
        "id": "NIUBN_Y-YLaP"
      },
      "execution_count": null,
      "outputs": [],
      "id": "NIUBN_Y-YLaP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Approach B: Financial dataset with sentiment scores"
      ],
      "metadata": {
        "id": "NaIs45u_YrYy"
      },
      "id": "NaIs45u_YrYy"
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the Variable selected financial dataset created in Appendix B\n",
        "\n",
        "# Read the pickle file of the cleaned headlines dataset created in Appendix A, into a dataframe\n",
        "\n",
        "Variable_financial = pd.read_pickle(\"Variable selected financial dataset\")\n",
        "\n",
        "# Renaming columns\n",
        "\n",
        "Variable_financial.rename(columns = {\"Instrument\": \"RIC\"}, inplace = True)\n"
      ],
      "metadata": {
        "id": "HmoF-1nRRkB7"
      },
      "id": "HmoF-1nRRkB7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Droping unneccessary index column\n",
        "\n",
        "Variable_financial.drop(['Index'],inplace = True, axis=1)\n"
      ],
      "metadata": {
        "id": "EUz2pPNaRkFH"
      },
      "id": "EUz2pPNaRkFH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Variable_financial"
      ],
      "metadata": {
        "id": "8uUEYt3bRkIi"
      },
      "id": "8uUEYt3bRkIi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Applying ML on full dataset with VADER compound scores"
      ],
      "metadata": {
        "id": "ISjHk0AdvSJe"
      },
      "id": "ISjHk0AdvSJe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VADER"
      ],
      "metadata": {
        "id": "FyMEQizLZsmC"
      },
      "id": "FyMEQizLZsmC"
    },
    {
      "cell_type": "code",
      "source": [
        "# Merging the VADER scores dataset together with the financial datasite on column RIC\n",
        "\n",
        "Vader_num_full_dataset = Variable_financial.merge(Headlines_Vader_ok[['Compound', 'RIC']], on = 'RIC')"
      ],
      "metadata": {
        "id": "JXh69cSiZzM6"
      },
      "execution_count": null,
      "outputs": [],
      "id": "JXh69cSiZzM6"
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing duplicated in column RIC\n",
        "\n",
        "Vader_num_full_dataset.drop_duplicates(subset=['RIC'], inplace = True)"
      ],
      "metadata": {
        "id": "amQAUDqDcmai"
      },
      "id": "amQAUDqDcmai",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Droping unneccessary RIC column\n",
        "\n",
        "Vader_num_full_dataset.drop(['RIC'],inplace = True, axis=1)\n"
      ],
      "metadata": {
        "id": "FoPebISwcsuM"
      },
      "id": "FoPebISwcsuM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Vader_num_full_dataset"
      ],
      "metadata": {
        "id": "413bEwi_dWA3"
      },
      "id": "413bEwi_dWA3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SVUHIo2k1nE"
      },
      "outputs": [],
      "source": [
        "# Choosing a subset of the data to split in between train and test:\n",
        "\n",
        "X = Vader_num_full_dataset\n",
        "y = Vader_num_full_dataset['Target/Non-Target']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size=0.3, random_state=11)"
      ],
      "id": "5SVUHIo2k1nE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cePjVZ3k1nG"
      },
      "outputs": [],
      "source": [
        "# BASELINE MODEL logistic regression\n",
        "\n",
        "# Baseline performance 2: Logistic regression classifier\n",
        "\n",
        "lr = LogisticRegression()\n",
        "\n",
        "# Applying the model to the training data:\n",
        "\n",
        "lr.fit(X_train,y_train)\n",
        "\n",
        "\n",
        "# Predict the test model:\n",
        "labels_lr = lr.predict(X_test)\n"
      ],
      "id": "6cePjVZ3k1nG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrvVzwLVk1nG"
      },
      "outputs": [],
      "source": [
        "# Let's evaluate the results with accuracy:\n",
        "\n",
        "print('Logistic Regression Test Accuracy:', accuracy_score(y_test, labels_lr))\n",
        "print('Logistic Regression Train Accuracy:', accuracy_score(y_train, lr.predict(X_train)))\n",
        "\n",
        "# Recall - but also precision, f1-score and support:\n",
        "\n",
        "print(classification_report(y_test, labels_lr))\n",
        "print(classification_report(y_train, lr.predict(X_train)))\n",
        "\n",
        "# Confusion matrix:\n",
        "\n",
        "mat_lr = confusion_matrix(y_test,labels_lr)\n",
        "sns.heatmap(mat_lr, square=True, annot=True, fmt=\"d\", cbar=False,\n",
        "           xticklabels=['Non-Target', 'Target'], yticklabels=['Non-Target', 'Target'])\n",
        "\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('Label')"
      ],
      "id": "qrvVzwLVk1nG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEqz7_N-k1nH"
      },
      "outputs": [],
      "source": [
        "# Precision\n",
        "\n",
        "precision = precision_score(y_test, labels_lr, average=None)\n",
        "print(precision)\n",
        "\n",
        "# Recall\n",
        "\n",
        "recall = recall_score(y_test, labels_lr, average=None)\n",
        "print(recall)\n",
        "\n",
        "# F-score\n",
        "f_score = f1_score(y_test, labels_lr, average=None)\n",
        "print(f_score)"
      ],
      "id": "mEqz7_N-k1nH"
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying nested cross-validation check:\n",
        "scores_lr = cross_val_score(lr, X, y, cv=10, scoring='accuracy')\n",
        "print(scores_lr)\n",
        "print(\"%0.4f accuracy with a standard deviation of %0.4f\" % (scores_lr.mean(), scores_lr.std()))"
      ],
      "metadata": {
        "id": "ygn6NJYZmchI"
      },
      "id": "ygn6NJYZmchI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UJdVOsLk1nH"
      },
      "outputs": [],
      "source": [
        "# Random Forest on feature selected dataset with Target\n",
        "\n",
        "# Create regressor object\n",
        "    \n",
        "regressor = RandomForestRegressor(n_estimators = 100, random_state = 0)\n",
        "  \n",
        "# Applying the model to the training data:\n",
        "\n",
        "regressor.fit(X_train,y_train)\n",
        "\n",
        "# Predict the test model:\n",
        "\n",
        "labels_regressor = regressor.predict(X_test)\n",
        "\n"
      ],
      "id": "1UJdVOsLk1nH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHPSgVufk1nH"
      },
      "outputs": [],
      "source": [
        "# Let's evalueate the results with accuracy:\n",
        "\n",
        "print('Random Forest Test Accuracy:', accuracy_score(y_test, labels_regressor))\n",
        "print('Random Forest Train Accuracy:', accuracy_score(y_train, regressor.predict(X_train)))\n",
        "\n",
        "# Recall - but also precision, f1-score and support:\n",
        "\n",
        "print(classification_report(y_test, labels_regressor))\n",
        "print(classification_report(y_train, regressor.predict(X_train)))\n",
        "\n",
        "# Confusion matrix:\n",
        "\n",
        "mat_regressor = confusion_matrix(y_test,labels_regressor)\n",
        "sns.heatmap(mat_regressor, square=True, annot=True, fmt=\"d\", cbar=False,\n",
        "           xticklabels=['Non-Target', 'Target'], yticklabels=['Non-Target', 'Target'])\n",
        "\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('Label')"
      ],
      "id": "SHPSgVufk1nH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNCI2DAhk1nI"
      },
      "outputs": [],
      "source": [
        "# Precision\n",
        "\n",
        "precision = precision_score(y_test, labels_regressor, average=None)\n",
        "print(precision)\n",
        "\n",
        "# Recall\n",
        "\n",
        "recall = recall_score(y_test, labels_regressor, average=None)\n",
        "print(recall)\n",
        "\n",
        "# F-score\n",
        "f_score = f1_score(y_test, labels_regressor, average=None)\n",
        "print(f_score)"
      ],
      "id": "tNCI2DAhk1nI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mj_jkA1Ck1nI"
      },
      "outputs": [],
      "source": [
        "# Neural Network NN\n",
        "# Building the classifier\n",
        "\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(13,13,13),max_iter=500)\n",
        "\n",
        "# Applying the model to the training data:\n",
        "\n",
        "mlp.fit(X_train,y_train)\n",
        "\n",
        "# Predict the test model:\n",
        "\n",
        "labels_mlp = mlp.predict(X_test)\n"
      ],
      "id": "mj_jkA1Ck1nI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlb_vevdk1nI"
      },
      "outputs": [],
      "source": [
        "# Let's evalueate the results with accuracy:\n",
        "\n",
        "print('NN Test Accuracy:', accuracy_score(y_test, labels_mlp))\n",
        "print('NN Train Accuracy:', accuracy_score(y_train, mlp.predict(X_train)))\n",
        "\n",
        "# Recall - but also precision, f1-score and support:\n",
        "\n",
        "print(classification_report(y_test, labels_mlp))\n",
        "print(classification_report(y_train, mlp.predict(X_train)))\n",
        "\n",
        "# Confusion matrix:\n",
        "\n",
        "mat_mlp = confusion_matrix(y_test,labels_mlp)\n",
        "sns.heatmap(mat_mlp, square=True, annot=True, fmt=\"d\", cbar=False,\n",
        "           xticklabels=['Non-Target', 'Target'], yticklabels=['Non-Target', 'Target'])\n",
        "\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('Label')\n"
      ],
      "id": "nlb_vevdk1nI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYupOihrk1nI"
      },
      "outputs": [],
      "source": [
        "# Applying nested cross-validation check:\n",
        "scores_mlp = cross_val_score(mlp, X, y, cv=10, scoring='accuracy')\n",
        "print(scores_mlp)\n",
        "print(\"%0.4f accuracy with a standard deviation of %0.4f\" % (scores_mlp.mean(), scores_mlp.std()))"
      ],
      "id": "KYupOihrk1nI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12UgRbzAk1nI"
      },
      "outputs": [],
      "source": [
        "# Precision\n",
        "\n",
        "precision = precision_score(y_test, labels_mlp, average=None)\n",
        "print(precision)\n",
        "\n",
        "# Recall\n",
        "\n",
        "recall = recall_score(y_test, labels_mlp, average=None)\n",
        "print(recall)\n",
        "\n",
        "# F-score\n",
        "f_score = f1_score(y_test, labels_mlp, average=None)\n",
        "print(f_score)"
      ],
      "id": "12UgRbzAk1nI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TV95ghHVk1nJ"
      },
      "outputs": [],
      "source": [
        "# Decision Tree\n",
        "\n",
        "dt = DecisionTreeClassifier(criterion='entropy')\n",
        "\n",
        "# Applying the model to the training data:\n",
        "\n",
        "dt.fit(X_train,y_train)\n",
        "\n",
        "# Predict the test model:\n",
        "\n",
        "labels_dt = dt.predict(X_test)\n"
      ],
      "id": "TV95ghHVk1nJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3I0iNYVrk1nJ"
      },
      "outputs": [],
      "source": [
        "# Let's evalueate the results with accuracy:\n",
        "\n",
        "print('Decision Tree Test Accuracy:', accuracy_score(y_test, labels_dt))\n",
        "print('Decision Tree Train Accuracy:', accuracy_score(y_train, dt.predict(X_train)))\n",
        "\n",
        "# Recall - but also precision, f1-score and support:\n",
        "\n",
        "print(classification_report(y_test, labels_dt))\n",
        "print(classification_report(y_train, dt.predict(X_train)))\n",
        "\n",
        "# Confusion matrix:\n",
        "\n",
        "mat_dt = confusion_matrix(y_test,labels_dt)\n",
        "sns.heatmap(mat_dt, square=True, annot=True, fmt=\"d\", cbar=False,\n",
        "           xticklabels=['Non-Target', 'Target'], yticklabels=['Non-Target', 'Target'])\n",
        "\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('Label')\n"
      ],
      "id": "3I0iNYVrk1nJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgguBfHxk1nJ"
      },
      "outputs": [],
      "source": [
        "# Precision\n",
        "\n",
        "precision = precision_score(y_test, labels_dt, average=None)\n",
        "print(precision)\n",
        "\n",
        "# Recall\n",
        "\n",
        "recall = recall_score(y_test, labels_dt, average=None)\n",
        "print(recall)\n",
        "\n",
        "# F-score\n",
        "f_score = f1_score(y_test, labels_dt, average=None)\n",
        "print(f_score)"
      ],
      "id": "CgguBfHxk1nJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piTxIuhnk1nJ"
      },
      "outputs": [],
      "source": [
        "# Support Vector Machine (SVM)\n",
        "\n",
        "# Building the linear Support Vector Machine Classifier\n",
        "\n",
        "Svm = LinearSVC(dual = False, random_state = 0, penalty = 'l1',tol = 1e-5)\n",
        "\n",
        "Svm.fit(X_train,y_train) \n",
        "\n",
        "# Predict the test model:\n",
        "\n",
        "labels_svm = Svm.predict(X_test)\n"
      ],
      "id": "piTxIuhnk1nJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_IK3qIY2k1nK"
      },
      "outputs": [],
      "source": [
        "# Let's evalueate the results with accuracy:\n",
        "\n",
        "print('SVM Test Accuracy:', accuracy_score(y_test, labels_svm))\n",
        "print('SVM Train Accuracy:', accuracy_score(y_train, Svm.predict(X_train)))\n",
        "\n",
        "# Recall - but also precision, f1-score and support:\n",
        "\n",
        "print(classification_report(y_test, labels_svm))\n",
        "print(classification_report(y_train, Svm.predict(X_train)))\n",
        "\n",
        "# Confusion matrix:\n",
        "\n",
        "mat_svm = confusion_matrix(y_test,labels_svm)\n",
        "sns.heatmap(mat_svm, square=True, annot=True, fmt=\"d\", cbar=False,\n",
        "           xticklabels=['Non-Terget', 'Terget'], yticklabels=['Non-Terget', 'Terget'])\n",
        "\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('Label')\n"
      ],
      "id": "_IK3qIY2k1nK"
    },
    {
      "cell_type": "code",
      "source": [
        "# Precision\n",
        "\n",
        "precision = precision_score(y_test, labels_svm, average=None)\n",
        "print(precision)\n",
        "\n",
        "# Recall\n",
        "\n",
        "recall = recall_score(y_test, labels_svm, average=None)\n",
        "print(recall)\n",
        "\n",
        "# F-score\n",
        "f_score = f1_score(y_test, labels_svm, average=None)\n",
        "print(f_score)"
      ],
      "metadata": {
        "id": "HEcGS8U6k1nK"
      },
      "execution_count": null,
      "outputs": [],
      "id": "HEcGS8U6k1nK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Applying ML on full dataset with TEXTBLOB polarity and subjectivity scores"
      ],
      "metadata": {
        "id": "7EN3ggbvvvak"
      },
      "id": "7EN3ggbvvvak"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TEXTBLOB"
      ],
      "metadata": {
        "id": "DXHuvMVSd3SZ"
      },
      "id": "DXHuvMVSd3SZ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Merging the TEXTBLOB scores dataset together with the financial datasite on column RIC\n",
        "\n",
        "Textblob_num_full_dataset = Variable_financial.merge(df[['Polarity','Subjectivity','RIC']], on = 'RIC')"
      ],
      "metadata": {
        "id": "6dP-Q9pSd45D"
      },
      "execution_count": null,
      "outputs": [],
      "id": "6dP-Q9pSd45D"
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing duplicated in column RIC\n",
        "\n",
        "Textblob_num_full_dataset.drop_duplicates(subset=['RIC'], inplace = True)"
      ],
      "metadata": {
        "id": "r5kmRiVpd45E"
      },
      "execution_count": null,
      "outputs": [],
      "id": "r5kmRiVpd45E"
    },
    {
      "cell_type": "code",
      "source": [
        "# Droping unneccessary RIC column\n",
        "\n",
        "Textblob_num_full_dataset.drop(['RIC'],inplace = True, axis=1)\n"
      ],
      "metadata": {
        "id": "WcQjfCrHd45E"
      },
      "execution_count": null,
      "outputs": [],
      "id": "WcQjfCrHd45E"
    },
    {
      "cell_type": "code",
      "source": [
        "Textblob_num_full_dataset"
      ],
      "metadata": {
        "id": "KOlOplOpd45E"
      },
      "execution_count": null,
      "outputs": [],
      "id": "KOlOplOpd45E"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smjCdmR4lNja"
      },
      "outputs": [],
      "source": [
        "# Choosing a subset of the data to split in between train and test:\n",
        "\n",
        "X = Textblob_num_full_dataset\n",
        "y = Textblob_num_full_dataset['Target/Non-Target']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size=0.3, random_state=11)"
      ],
      "id": "smjCdmR4lNja"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYgOAaAxlNjb"
      },
      "outputs": [],
      "source": [
        "# BASELINE MODEL logistic regression\n",
        "\n",
        "# Baseline performance 2: Logistic regression classifier\n",
        "\n",
        "lr = LogisticRegression()\n",
        "\n",
        "# Applying the model to the training data:\n",
        "\n",
        "lr.fit(X_train,y_train)\n",
        "\n",
        "\n",
        "# Predict the test model:\n",
        "labels_lr = lr.predict(X_test)\n"
      ],
      "id": "AYgOAaAxlNjb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYsupVs7lNjb"
      },
      "outputs": [],
      "source": [
        "# Let's evaluate the results with accuracy:\n",
        "\n",
        "print('Logistic Regression Test Accuracy:', accuracy_score(y_test, labels_lr))\n",
        "print('Logistic Regression Train Accuracy:', accuracy_score(y_train, lr.predict(X_train)))\n",
        "\n",
        "# Recall - but also precision, f1-score and support:\n",
        "\n",
        "print(classification_report(y_test, labels_lr))\n",
        "print(classification_report(y_train, lr.predict(X_train)))\n",
        "\n",
        "# Confusion matrix:\n",
        "\n",
        "mat_lr = confusion_matrix(y_test,labels_lr)\n",
        "sns.heatmap(mat_lr, square=True, annot=True, fmt=\"d\", cbar=False,\n",
        "           xticklabels=['Non-Target', 'Target'], yticklabels=['Non-Target', 'Target'])\n",
        "\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('Label')"
      ],
      "id": "xYsupVs7lNjb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ywo8jnUhlNjb"
      },
      "outputs": [],
      "source": [
        "# Precision\n",
        "\n",
        "precision = precision_score(y_test, labels_lr, average=None)\n",
        "print(precision)\n",
        "\n",
        "# Recall\n",
        "\n",
        "recall = recall_score(y_test, labels_lr, average=None)\n",
        "print(recall)\n",
        "\n",
        "# F-score\n",
        "f_score = f1_score(y_test, labels_lr, average=None)\n",
        "print(f_score)"
      ],
      "id": "Ywo8jnUhlNjb"
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying nested cross-validation check:\n",
        "scores_lr = cross_val_score(lr, X, y, cv=10, scoring='accuracy')\n",
        "print(scores_lr)\n",
        "print(\"%0.4f accuracy with a standard deviation of %0.4f\" % (scores_lr.mean(), scores_lr.std()))"
      ],
      "metadata": {
        "id": "L1THIKldqpTX"
      },
      "id": "L1THIKldqpTX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJqS6dbMlNjc"
      },
      "outputs": [],
      "source": [
        "# Random Forest on feature selected dataset with Target\n",
        "\n",
        "# Create regressor object\n",
        "    \n",
        "regressor = RandomForestRegressor(n_estimators = 100, random_state = 0)\n",
        "  \n",
        "# Applying the model to the training data:\n",
        "\n",
        "regressor.fit(X_train,y_train)\n",
        "\n",
        "# Predict the test model:\n",
        "\n",
        "labels_regressor = regressor.predict(X_test)\n",
        "\n"
      ],
      "id": "sJqS6dbMlNjc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXgDY741lNjc"
      },
      "outputs": [],
      "source": [
        "# Let's evalueate the results with accuracy:\n",
        "\n",
        "print('Random Forest Test Accuracy:', accuracy_score(y_test, labels_regressor))\n",
        "print('Random Forest Train Accuracy:', accuracy_score(y_train, regressor.predict(X_train)))\n",
        "\n",
        "# Recall - but also precision, f1-score and support:\n",
        "\n",
        "print(classification_report(y_test, labels_regressor))\n",
        "print(classification_report(y_train, regressor.predict(X_train)))\n",
        "\n",
        "# Confusion matrix:\n",
        "\n",
        "mat_regressor = confusion_matrix(y_test,labels_regressor)\n",
        "sns.heatmap(mat_regressor, square=True, annot=True, fmt=\"d\", cbar=False,\n",
        "           xticklabels=['Non-Target', 'Target'], yticklabels=['Non-Target', 'Target'])\n",
        "\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('Label')"
      ],
      "id": "ZXgDY741lNjc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mn6G6UX3lNjc"
      },
      "outputs": [],
      "source": [
        "# Precision\n",
        "\n",
        "precision = precision_score(y_test, labels_regressor, average=None)\n",
        "print(precision)\n",
        "\n",
        "# Recall\n",
        "\n",
        "recall = recall_score(y_test, labels_regressor, average=None)\n",
        "print(recall)\n",
        "\n",
        "# F-score\n",
        "f_score = f1_score(y_test, labels_regressor, average=None)\n",
        "print(f_score)"
      ],
      "id": "mn6G6UX3lNjc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mvPI2N2lNjc"
      },
      "outputs": [],
      "source": [
        "# Neural Network NN\n",
        "# Building the classifier\n",
        "\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(13,13,13),max_iter=500)\n",
        "\n",
        "# Applying the model to the training data:\n",
        "\n",
        "mlp.fit(X_train,y_train)\n",
        "\n",
        "# Predict the test model:\n",
        "\n",
        "labels_mlp = mlp.predict(X_test)\n"
      ],
      "id": "9mvPI2N2lNjc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJLXqfWxlNjc"
      },
      "outputs": [],
      "source": [
        "# Let's evalueate the results with accuracy:\n",
        "\n",
        "print('NN Test Accuracy:', accuracy_score(y_test, labels_mlp))\n",
        "print('NN Train Accuracy:', accuracy_score(y_train, mlp.predict(X_train)))\n",
        "\n",
        "# Recall - but also precision, f1-score and support:\n",
        "\n",
        "print(classification_report(y_test, labels_mlp))\n",
        "print(classification_report(y_train, mlp.predict(X_train)))\n",
        "\n",
        "# Confusion matrix:\n",
        "\n",
        "mat_mlp = confusion_matrix(y_test,labels_mlp)\n",
        "sns.heatmap(mat_mlp, square=True, annot=True, fmt=\"d\", cbar=False,\n",
        "           xticklabels=['Non-Target', 'Target'], yticklabels=['Non-Target', 'Target'])\n",
        "\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('Label')\n"
      ],
      "id": "OJLXqfWxlNjc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k69v7b5ilNjc"
      },
      "outputs": [],
      "source": [
        "# Applying nested cross-validation check:\n",
        "scores_mlp = cross_val_score(mlp, X, y, cv=10, scoring='accuracy')\n",
        "print(scores_mlp)\n",
        "print(\"%0.4f accuracy with a standard deviation of %0.4f\" % (scores_mlp.mean(), scores_mlp.std()))"
      ],
      "id": "k69v7b5ilNjc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPTUMFp7lNjc"
      },
      "outputs": [],
      "source": [
        "# Precision\n",
        "\n",
        "precision = precision_score(y_test, labels_mlp, average=None)\n",
        "print(precision)\n",
        "\n",
        "# Recall\n",
        "\n",
        "recall = recall_score(y_test, labels_mlp, average=None)\n",
        "print(recall)\n",
        "\n",
        "# F-score\n",
        "f_score = f1_score(y_test, labels_mlp, average=None)\n",
        "print(f_score)"
      ],
      "id": "VPTUMFp7lNjc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtN9JyS5lNjd"
      },
      "outputs": [],
      "source": [
        "# Decision Tree\n",
        "\n",
        "dt = DecisionTreeClassifier(criterion='entropy')\n",
        "\n",
        "# Applying the model to the training data:\n",
        "\n",
        "dt.fit(X_train,y_train)\n",
        "\n",
        "# Predict the test model:\n",
        "\n",
        "labels_dt = dt.predict(X_test)\n"
      ],
      "id": "WtN9JyS5lNjd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yl-xGHmXlNjd"
      },
      "outputs": [],
      "source": [
        "# Let's evalueate the results with accuracy:\n",
        "\n",
        "print('Decision Tree Test Accuracy:', accuracy_score(y_test, labels_dt))\n",
        "print('Decision Tree Train Accuracy:', accuracy_score(y_train, dt.predict(X_train)))\n",
        "\n",
        "# Recall - but also precision, f1-score and support:\n",
        "\n",
        "print(classification_report(y_test, labels_dt))\n",
        "print(classification_report(y_train, dt.predict(X_train)))\n",
        "\n",
        "# Confusion matrix:\n",
        "\n",
        "mat_dt = confusion_matrix(y_test,labels_dt)\n",
        "sns.heatmap(mat_dt, square=True, annot=True, fmt=\"d\", cbar=False,\n",
        "           xticklabels=['Non-Target', 'Target'], yticklabels=['Non-Target', 'Target'])\n",
        "\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('Label')\n"
      ],
      "id": "Yl-xGHmXlNjd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gixsfiIylNjd"
      },
      "outputs": [],
      "source": [
        "# Precision\n",
        "\n",
        "precision = precision_score(y_test, labels_dt, average=None)\n",
        "print(precision)\n",
        "\n",
        "# Recall\n",
        "\n",
        "recall = recall_score(y_test, labels_dt, average=None)\n",
        "print(recall)\n",
        "\n",
        "# F-score\n",
        "f_score = f1_score(y_test, labels_dt, average=None)\n",
        "print(f_score)"
      ],
      "id": "gixsfiIylNjd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Kj1OY8_lNjd"
      },
      "outputs": [],
      "source": [
        "# Support Vector Machine (SVM)\n",
        "\n",
        "# Building the linear Support Vector Machine Classifier\n",
        "\n",
        "Svm = LinearSVC(dual = False, random_state = 0, penalty = 'l1',tol = 1e-5)\n",
        "\n",
        "Svm.fit(X_train,y_train) \n",
        "\n",
        "# Predict the test model:\n",
        "\n",
        "labels_svm = Svm.predict(X_test)\n"
      ],
      "id": "1Kj1OY8_lNjd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VhfIoD6lNjd"
      },
      "outputs": [],
      "source": [
        "# Let's evalueate the results with accuracy:\n",
        "\n",
        "print('SVM Test Accuracy:', accuracy_score(y_test, labels_svm))\n",
        "print('SVM Train Accuracy:', accuracy_score(y_train, Svm.predict(X_train)))\n",
        "\n",
        "# Recall - but also precision, f1-score and support:\n",
        "\n",
        "print(classification_report(y_test, labels_svm))\n",
        "print(classification_report(y_train, Svm.predict(X_train)))\n",
        "\n",
        "# Confusion matrix:\n",
        "\n",
        "mat_svm = confusion_matrix(y_test,labels_svm)\n",
        "sns.heatmap(mat_svm, square=True, annot=True, fmt=\"d\", cbar=False,\n",
        "           xticklabels=['Non-Terget', 'Terget'], yticklabels=['Non-Terget', 'Terget'])\n",
        "\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('Label')\n"
      ],
      "id": "-VhfIoD6lNjd"
    },
    {
      "cell_type": "code",
      "source": [
        "# Precision\n",
        "\n",
        "precision = precision_score(y_test, labels_svm, average=None)\n",
        "print(precision)\n",
        "\n",
        "# Recall\n",
        "\n",
        "recall = recall_score(y_test, labels_svm, average=None)\n",
        "print(recall)\n",
        "\n",
        "# F-score\n",
        "f_score = f1_score(y_test, labels_svm, average=None)\n",
        "print(f_score)"
      ],
      "metadata": {
        "id": "DPBELj0vlNjd"
      },
      "execution_count": null,
      "outputs": [],
      "id": "DPBELj0vlNjd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hypothesis testing: High share prices, together with no or very low positive news sentiment, is an indication of an imminent M&A announcement."
      ],
      "metadata": {
        "id": "vt3y5xU8KDAg"
      },
      "id": "vt3y5xU8KDAg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VADER"
      ],
      "metadata": {
        "id": "7Ba2KmA-PLtf"
      },
      "id": "7Ba2KmA-PLtf"
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtering the dataset for ontly Target companies\n",
        "\n",
        "Vader_share_target = Vader_share_price[Vader_share_price['Target/Non-Target'] == 1]"
      ],
      "metadata": {
        "id": "n-6iIXF-gOqf"
      },
      "id": "n-6iIXF-gOqf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling my dataset for better visualization\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(Vader_share_target)\n",
        "scaled = scaler.transform(Vader_share_target)\n",
        "scaled_df = pd.DataFrame(scaled, columns=Vader_share_target.columns)\n",
        "\n",
        "scaled_df"
      ],
      "metadata": {
        "id": "L0-t9paKIEef"
      },
      "id": "L0-t9paKIEef",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting VADER Compound scores together with Price To Sales Per Share (Daily Time Series Ratio)\n",
        "\n",
        "scaled_df.plot(x=\"Compound\", y=[\"Price To Sales Per Share (Daily Time Series Ratio)\"], kind=\"kde\", figsize=(9, 8))\n",
        "\n",
        "plt.xlabel('Compound scores', fontsize=10)\n",
        "plt.ylabel('Density', fontsize=10)\n",
        "plt.title('VADER Compound scores and Price To Sales Per Share (Daily Time Series Ratio)' , fontsize=20)\n",
        " \n"
      ],
      "metadata": {
        "id": "VTvv1fmuMRN9"
      },
      "id": "VTvv1fmuMRN9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting VADER Compound scores together with Price To Book Value Per Share (Daily Time Series Ratio) \n",
        "\n",
        "scaled_df.plot(x=\"Compound\", y=[\"Price To Book Value Per Share (Daily Time Series Ratio)\"], kind=\"kde\", figsize=(9, 8))\n",
        "\n",
        "plt.xlabel('Compound scores', fontsize=10)\n",
        "plt.ylabel('Density', fontsize=10)\n",
        "plt.title('VADER Compound scores and Price To Book Value Per Share (Daily Time Series Ratio)' , fontsize=20)\n",
        "plt.legend(bbox_to_anchor=(0.5, 0.9))"
      ],
      "metadata": {
        "id": "oO76H8_WNByv"
      },
      "id": "oO76H8_WNByv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TEXTBLOB"
      ],
      "metadata": {
        "id": "z_bQ1yeGPQfb"
      },
      "id": "z_bQ1yeGPQfb"
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtering the dataset for ontly Target companies\n",
        "\n",
        "Textblob_share_target = Textblob_share_price[Textblob_share_price['Target/Non-Target'] == 1]"
      ],
      "metadata": {
        "id": "qknJOvPJkJyq"
      },
      "id": "qknJOvPJkJyq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling my dataset for better visualization\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(Textblob_share_target)\n",
        "scaled = scaler.transform(Textblob_share_target)\n",
        "scaled_df = pd.DataFrame(scaled, columns=Textblob_share_target.columns)\n",
        "\n",
        "scaled_df"
      ],
      "metadata": {
        "id": "CGNDkLLhO7um"
      },
      "id": "CGNDkLLhO7um",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting TEXTBLOB Polarity and Subjectivity scores together with Price To Sales Per Share (Daily Time Series Ratio)\n",
        "\n",
        "scaled_df.plot(x=\"Polarity\", y=['Subjectivity', \"Price To Sales Per Share (Daily Time Series Ratio)\"], kind=\"kde\", figsize=(9, 8))\n",
        "\n",
        "plt.xlabel('Polarity scores', fontsize=10)\n",
        "plt.ylabel('Density', fontsize=10)\n",
        "plt.title('TEXTBLOB Polarity and Subjectivity scores and Price To Sales Per Share (Daily Time Series Ratio)' , fontsize=20)\n",
        " "
      ],
      "metadata": {
        "id": "twbyn9FoKkcU"
      },
      "id": "twbyn9FoKkcU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting TEXTBLOB Polarity and Subjectivity scores together with Price To Book Value Per Share (Daily Time Series Ratio)\n",
        "\n",
        "scaled_df.plot(x=\"Polarity\", y=['Subjectivity', \"Price To Sales Per Share (Daily Time Series Ratio)\"], kind=\"kde\", figsize=(9, 8))\n",
        "\n",
        "plt.xlabel('Polarity scores', fontsize=10)\n",
        "plt.ylabel('Density', fontsize=10)\n",
        "plt.title('TEXTBLOB Polarity and Subjectivity scores and Price To Book Value Per Share (Daily Time Series Ratio)' , fontsize=20)\n",
        " "
      ],
      "metadata": {
        "id": "fG2572txPuEi"
      },
      "id": "fG2572txPuEi",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}