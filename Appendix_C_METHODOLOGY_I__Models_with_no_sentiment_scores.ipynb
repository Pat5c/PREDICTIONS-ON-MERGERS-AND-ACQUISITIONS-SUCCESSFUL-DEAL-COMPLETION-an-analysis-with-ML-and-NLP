{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Appendix C: METHODOLOGY I â€“ Models with No sentiment scores"
      ],
      "metadata": {
        "id": "8zRCfCEkYQAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib \n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "uZb2ZbY7XNr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the pickle file of the variable selected dataset created in Appendix B, into a dataframe\n",
        "\n",
        "num_df_features = pd.read_pickle(\"Variable selected financial dataset\")\n",
        "\n",
        "# View the DataFrame\n",
        "num_df_features"
      ],
      "metadata": {
        "id": "TytmoVjLwBCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping unneccessary columns\n",
        "\n",
        "num_df_features.drop(['Index','Instrument'],inplace = True, axis=1)"
      ],
      "metadata": {
        "id": "PPNs6BlbjM66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yD2u_gNGKfZ"
      },
      "outputs": [],
      "source": [
        "num_df_features['Target/Non-Target'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjUWdD60GKfe"
      },
      "outputs": [],
      "source": [
        "# Choosing a subset of the data to split in between train and test:\n",
        "\n",
        "X = num_df_features\n",
        "y = num_df_features['Target/Non-Target']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size=0.3, random_state=11)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BASELINE MODEL: Logistic Regression"
      ],
      "metadata": {
        "id": "2SP1llySZBkb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWfqJvVaGKff"
      },
      "outputs": [],
      "source": [
        "# Baseline performance: Logistic regression classifier\n",
        "\n",
        "lr = LogisticRegression()\n",
        "\n",
        "# Applying the model to the training data:\n",
        "\n",
        "lr.fit(X_train,y_train)\n",
        "\n",
        "# Predict the test model:\n",
        "\n",
        "labels_lr = lr.predict(X_test)\n",
        "labels_lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXG2NMpHGKfg"
      },
      "outputs": [],
      "source": [
        "# Let's evaluate the results with accuracy:\n",
        "\n",
        "print('Logistic Regression Test Accuracy:', accuracy_score(y_test, labels_lr))\n",
        "print('Logistic Regression Train Accuracy:', accuracy_score(y_train, lr.predict(X_train)))\n",
        "\n",
        "# Recall - but also precision, f1-score and support:\n",
        "\n",
        "print(classification_report(y_test, labels_lr))\n",
        "print(classification_report(y_train, lr.predict(X_train)))\n",
        "\n",
        "# Confusion matrix:\n",
        "\n",
        "mat_lr = confusion_matrix(y_test,labels_lr)\n",
        "sns.heatmap(mat_lr, square=True, annot=True, fmt=\"d\", cbar=False,\n",
        "           xticklabels=['Non-Target', 'Target'], yticklabels=['Non-Target', 'Target'])\n",
        "\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('Label')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Precision\n",
        "\n",
        "precision = precision_score(y_test, labels_lr, average=None)\n",
        "print(precision)\n",
        "\n",
        "# Recall\n",
        "\n",
        "recall = recall_score(y_test, labels_lr, average=None)\n",
        "print(recall)\n",
        "\n",
        "# F-score\n",
        "f_score = f1_score(y_test, labels_lr, average=None)\n",
        "print(f_score)"
      ],
      "metadata": {
        "id": "vFFWCy7pucw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjNEiKjVGKfh"
      },
      "outputs": [],
      "source": [
        "# Applying nested cross-validation check:\n",
        "\n",
        "scores = cross_val_score(lr, X, y, cv=10, scoring='accuracy')\n",
        "print(scores)\n",
        "print(\"%0.4f accuracy with a standard deviation of %0.4f\" % (scores.mean(), scores.std()))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# THE OTHER CLASSIFICATION MODELS"
      ],
      "metadata": {
        "id": "bfWzNQ-yamiu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FndQZ5BGKfi"
      },
      "outputs": [],
      "source": [
        "# Random Forest model\n",
        "\n",
        "# Creating the regressor object\n",
        "    \n",
        "regressor = RandomForestRegressor(n_estimators = 100, random_state = 0)\n",
        "  \n",
        "# Applying the model to the training data:\n",
        "\n",
        "regressor.fit(X_train,y_train)\n",
        "\n",
        "# Predict the test model:\n",
        "\n",
        "labels_regressor = regressor.predict(X_test)\n",
        "labels_regressor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PqaHiU6GKfk"
      },
      "outputs": [],
      "source": [
        "# Let's evalueate the results with accuracy:\n",
        "\n",
        "print('Random Forest Test Accuracy:', accuracy_score(y_test, labels_regressor))\n",
        "print('Random Forest Train Accuracy:', accuracy_score(y_train, regressor.predict(X_train)))\n",
        "\n",
        "# Recall - but also precision, f1-score and support:\n",
        "\n",
        "print(classification_report(y_test, labels_regressor))\n",
        "print(classification_report(y_train, regressor.predict(X_train)))\n",
        "\n",
        "# Confusion matrix:\n",
        "\n",
        "mat_regressor = confusion_matrix(y_test,labels_regressor)\n",
        "sns.heatmap(mat_regressor, square=True, annot=True, fmt=\"d\", cbar=False,\n",
        "           xticklabels=['Non-Target', 'Target'], yticklabels=['Non-Target', 'Target'])\n",
        "\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('Label')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Precision\n",
        "\n",
        "precision = precision_score(y_test, labels_regressor, average=None)\n",
        "print(precision)\n",
        "\n",
        "# Recall\n",
        "\n",
        "recall = recall_score(y_test, labels_regressor, average=None)\n",
        "print(recall)\n",
        "\n",
        "# F-score\n",
        "f_score = f1_score(y_test, labels_regressor, average=None)\n",
        "print(f_score)"
      ],
      "metadata": {
        "id": "TdR4oI_gA_mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ah6yJ1m_GKfn"
      },
      "outputs": [],
      "source": [
        "# Neural Network (NN)\n",
        "\n",
        "# Building the classifier\n",
        "\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(13,13,13),max_iter=500)\n",
        "\n",
        "# Applying the model to the training data:\n",
        "\n",
        "mlp.fit(X_train,y_train)\n",
        "\n",
        "# Predict the test model:\n",
        "\n",
        "labels_mlp = mlp.predict(X_test)\n",
        "labels_mlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPp4iStCGKfo"
      },
      "outputs": [],
      "source": [
        "# Let's evalueate the results with accuracy:\n",
        "\n",
        "print('NN Test Accuracy:', accuracy_score(y_test, labels_mlp))\n",
        "print('NN Train Accuracy:', accuracy_score(y_train, mlp.predict(X_train)))\n",
        "\n",
        "# Recall - but also precision, f1-score and support:\n",
        "\n",
        "print(classification_report(y_test, labels_mlp))\n",
        "print(classification_report(y_train, mlp.predict(X_train)))\n",
        "\n",
        "# Confusion matrix:\n",
        "\n",
        "mat_mlp = confusion_matrix(y_test,labels_mlp)\n",
        "sns.heatmap(mat_mlp, square=True, annot=True, fmt=\"d\", cbar=False,\n",
        "           xticklabels=['Non-Target', 'Target'], yticklabels=['Non-Target', 'Target'])\n",
        "\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('Label')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Precision\n",
        "\n",
        "precision = precision_score(y_test, labels_mlp, average=None)\n",
        "print(precision)\n",
        "\n",
        "# Recall\n",
        "\n",
        "recall = recall_score(y_test, labels_mlp, average=None)\n",
        "print(recall)\n",
        "\n",
        "# F-score\n",
        "f_score = f1_score(y_test, labels_mlp, average=None)\n",
        "print(f_score)"
      ],
      "metadata": {
        "id": "mkF3op6hBQYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBJSCJQgGKfo"
      },
      "outputs": [],
      "source": [
        "# Applying nested cross-validation check:\n",
        "scores_mlp = cross_val_score(mlp, X, y, cv=10, scoring='accuracy')\n",
        "print(scores_mlp)\n",
        "print(\"%0.4f accuracy with a standard deviation of %0.4f\" % (scores_mlp.mean(), scores_mlp.std()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIzzq72jGKfq"
      },
      "outputs": [],
      "source": [
        "# Decision Tree\n",
        "\n",
        "# Building the model\n",
        "\n",
        "dt = DecisionTreeClassifier(criterion='entropy')\n",
        "\n",
        "# Applying the model to the training data:\n",
        "\n",
        "dt.fit(X_train,y_train)\n",
        "\n",
        "# Predict the test model:\n",
        "\n",
        "labels_dt = dt.predict(X_test)\n",
        "labels_dt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "KPTp2VvyGKfr"
      },
      "outputs": [],
      "source": [
        "# Let's evalueate the results with accuracy:\n",
        "\n",
        "print('Decision Tree Test Accuracy:', accuracy_score(y_test, labels_dt))\n",
        "print('Decision Tree Train Accuracy:', accuracy_score(y_train, dt.predict(X_train)))\n",
        "\n",
        "# Recall - but also precision, f1-score and support:\n",
        "\n",
        "print(classification_report(y_test, labels_dt))\n",
        "print(classification_report(y_train, dt.predict(X_train)))\n",
        "\n",
        "# Confusion matrix:\n",
        "\n",
        "mat_dt = confusion_matrix(y_test,labels_dt)\n",
        "sns.heatmap(mat_dt, square=True, annot=True, fmt=\"d\", cbar=False,\n",
        "           xticklabels=['Non-Target', 'Target'], yticklabels=['Non-Target', 'Target'])\n",
        "\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('Label')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Precision\n",
        "\n",
        "precision = precision_score(y_test, labels_dt, average=None)\n",
        "print(precision)\n",
        "\n",
        "# Recall\n",
        "\n",
        "recall = recall_score(y_test, labels_dt, average=None)\n",
        "print(recall)\n",
        "\n",
        "# F-score\n",
        "f_score = f1_score(y_test, labels_dt, average=None)\n",
        "print(f_score)"
      ],
      "metadata": {
        "id": "LI45efWsDHv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Support Vector Machine (SVM)\n",
        "\n",
        "# Building the linear Support Vector Machine Classifier\n",
        "\n",
        "Svm = LinearSVC(dual = False, random_state = 0, penalty = 'l1',tol = 1e-5)\n",
        "\n",
        "Svm.fit(X_train,y_train) \n",
        "\n",
        "# Predict the test model:\n",
        "\n",
        "labels_svm = Svm.predict(X_test)\n",
        "labels_svm"
      ],
      "metadata": {
        "id": "PaOSVBvLjEje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owGNR_rXGKfs"
      },
      "outputs": [],
      "source": [
        "# Let's evalueate the results with accuracy:\n",
        "\n",
        "print('SVM Test Accuracy:', accuracy_score(y_test, labels_svm))\n",
        "print('SVM Train Accuracy:', accuracy_score(y_train, Svm.predict(X_train)))\n",
        "\n",
        "# Recall - but also precision, f1-score and support:\n",
        "\n",
        "print(classification_report(y_test, labels_svm))\n",
        "print(classification_report(y_train, Svm.predict(X_train)))\n",
        "\n",
        "# Confusion matrix:\n",
        "\n",
        "mat_svm = confusion_matrix(y_test,labels_svm)\n",
        "sns.heatmap(mat_svm, square=True, annot=True, fmt=\"d\", cbar=False,\n",
        "           xticklabels=['Non-Terget', 'Terget'], yticklabels=['Non-Terget', 'Terget'])\n",
        "\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('Label')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dxz2LgwcGKfs"
      },
      "outputs": [],
      "source": [
        "# Precision\n",
        "\n",
        "precision = precision_score(y_test, labels_svm, average=None)\n",
        "print(precision)\n",
        "\n",
        "# Recall\n",
        "\n",
        "recall = recall_score(y_test, labels_svm, average=None)\n",
        "print(recall)\n",
        "\n",
        "# F-score\n",
        "f_score = f1_score(y_test, labels_svm, average=None)\n",
        "print(f_score)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}